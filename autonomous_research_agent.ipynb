{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AAI-520 Final Project: Autonomous Investment Research Agent\n",
    "\n",
    "**Powered by Agentic AI with Multi-Workflow Patterns**\n",
    "\n",
    "This notebook demonstrates an autonomous investment research agent that:\n",
    "1. Plans its research steps for a given stock\n",
    "2. Uses tools dynamically (APIs, datasets, retrieval)\n",
    "3. Self-reflects to assess output quality\n",
    "4. Learns across runs (memory/notes for improvement)\n",
    "\n",
    "## Three Core Workflow Patterns\n",
    "- **Prompt Chaining**: Sequential processing pipeline (Ingest → Preprocess → Classify → Extract → Summarize)\n",
    "- **Routing**: Intelligent task distribution to specialist agents\n",
    "- **Evaluator-Optimizer**: Iterative self-improvement through evaluation and refinement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "import requests\n",
    "from typing import Dict, List, Optional, Literal, Set\n",
    "from textwrap import dedent\n",
    "\n",
    "# Pydantic for data models\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# LangChain imports\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_community.tools.yahoo_finance_news import YahooFinanceNewsTool\n",
    "from langchain_community.tools import DuckDuckGoSearchResults, WikipediaQueryRun\n",
    "from langchain_community.utilities import WikipediaAPIWrapper\n",
    "\n",
    "# NetworkX for knowledge graphs\n",
    "import networkx as nx\n",
    "\n",
    "print(\"✓ All imports loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM Configuration\n",
    "LMSTUDIO_URL = \"http://localhost:1234/v1\"\n",
    "LLM_NAME = \"gemma-3-27b-it-qat\"  # Match the actual model ID from LM Studio\n",
    "ENTITY_EXTRACTOR = \"gemma-3-27b-it-qat\"  # Use same model for extraction\n",
    "EXTRACTION_TEMPERATURE = 0\n",
    "\n",
    "# API Keys\n",
    "ALPHA_VANTAGE_API_KEY = os.getenv('ALPHA_VANTAGE_KEY')\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "# Data Fetching\n",
    "FETCH_DELAY_SECONDS = 1.5\n",
    "\n",
    "# Database\n",
    "SQLITE_DB_PATH = \"kg_store.db\"\n",
    "MEMORY_FILE = \"agent_memory.txt\"\n",
    "\n",
    "# Financial Entity Labels\n",
    "FINANCIAL_ENTITY_LABELS = {\n",
    "    \"ORGANIZATION\", \"PERSON\", \"PRODUCT\", \"EVENT\",\n",
    "    \"STOCK_SYMBOL\", \"POLICY\", \"GOVERNMENT\", \"COMPANY\"\n",
    "}\n",
    "\n",
    "print(\"✓ Configuration loaded\")\n",
    "print(f\"  LLM: {LLM_NAME}\")\n",
    "print(f\"  LM Studio URL: {LMSTUDIO_URL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Entity Models ===\n",
    "class Entity(BaseModel):\n",
    "    \"\"\"Named entity extracted from text.\"\"\"\n",
    "    text: str = Field(description=\"The extracted entity text\")\n",
    "    label: str = Field(description=\"Entity type (PERSON, ORGANIZATION, STOCK_SYMBOL, etc.)\")\n",
    "    confidence: Optional[float] = Field(description=\"Confidence score\", default=None)\n",
    "\n",
    "\n",
    "class NERResponse(BaseModel):\n",
    "    \"\"\"Named Entity Recognition response.\"\"\"\n",
    "    entities: List[Entity] = Field(description=\"List of extracted named entities\")\n",
    "\n",
    "\n",
    "# === Prompt Chaining Models ===\n",
    "class NewsArticle(BaseModel):\n",
    "    \"\"\"Raw news article model.\"\"\"\n",
    "    title: str\n",
    "    content: str\n",
    "    source: str\n",
    "    timestamp: Optional[str] = None\n",
    "\n",
    "\n",
    "class PreprocessedNews(BaseModel):\n",
    "    \"\"\"Cleaned and normalized news article.\"\"\"\n",
    "    title: str\n",
    "    cleaned_content: str\n",
    "    source: str\n",
    "    word_count: int\n",
    "    contains_financials: bool\n",
    "\n",
    "\n",
    "class NewsClassification(BaseModel):\n",
    "    \"\"\"Classification result for news article.\"\"\"\n",
    "    category: str = Field(description=\"Category: earnings, market_analysis, policy, merger_acquisition, general\")\n",
    "    sentiment: str = Field(description=\"Sentiment: positive, negative, neutral\")\n",
    "    relevance_score: float = Field(description=\"Relevance score 0-1\")\n",
    "    confidence: float = Field(description=\"Classification confidence 0-1\")\n",
    "\n",
    "\n",
    "class ExtractedEntities(BaseModel):\n",
    "    \"\"\"Extracted entities and key facts from news.\"\"\"\n",
    "    companies: List[str] = Field(description=\"Company names mentioned\")\n",
    "    people: List[str] = Field(description=\"Key people mentioned\")\n",
    "    financial_metrics: List[str] = Field(description=\"Financial metrics (revenue, EPS, etc.)\")\n",
    "    key_events: List[str] = Field(description=\"Important events or announcements\")\n",
    "    stock_symbols: List[str] = Field(description=\"Stock ticker symbols\")\n",
    "\n",
    "\n",
    "class NewsSummary(BaseModel):\n",
    "    \"\"\"Final summarized output.\"\"\"\n",
    "    executive_summary: str = Field(description=\"2-3 sentence summary\")\n",
    "    key_points: List[str] = Field(description=\"Bullet points of key takeaways\")\n",
    "    investment_implications: str = Field(description=\"What this means for investors\")\n",
    "    entities: ExtractedEntities\n",
    "    classification: NewsClassification\n",
    "\n",
    "\n",
    "# === Routing Models ===\n",
    "class RoutingDecision(BaseModel):\n",
    "    \"\"\"Router's decision on where to send content.\"\"\"\n",
    "    route: Literal[\"earnings\", \"news\", \"market\"] = Field(\n",
    "        description=\"Which specialist to route to: earnings, news, or market\"\n",
    "    )\n",
    "    confidence: float = Field(description=\"Confidence in routing decision (0-1)\")\n",
    "    reasoning: str = Field(description=\"Brief explanation of routing choice\")\n",
    "\n",
    "\n",
    "class EarningsAnalysis(BaseModel):\n",
    "    \"\"\"Output from the earnings specialist.\"\"\"\n",
    "    revenue_analysis: str = Field(description=\"Analysis of revenue performance\")\n",
    "    profitability_analysis: str = Field(description=\"Analysis of profit margins and EPS\")\n",
    "    growth_trends: str = Field(description=\"YoY and QoQ growth trends\")\n",
    "    guidance_assessment: str = Field(description=\"Assessment of forward guidance\")\n",
    "    key_metrics: List[str] = Field(description=\"Key financial metrics extracted\")\n",
    "    recommendation: str = Field(description=\"Investment recommendation\")\n",
    "\n",
    "\n",
    "class NewsAnalysis(BaseModel):\n",
    "    \"\"\"Output from the news specialist.\"\"\"\n",
    "    event_summary: str = Field(description=\"Summary of the news event\")\n",
    "    market_impact: str = Field(description=\"Potential market impact analysis\")\n",
    "    stakeholder_analysis: str = Field(description=\"Who is affected and how\")\n",
    "    timeline: str = Field(description=\"Timeline of events or expected developments\")\n",
    "    credibility_score: float = Field(description=\"News source credibility (0-1)\")\n",
    "    actionable_insights: List[str] = Field(description=\"Actionable takeaways\")\n",
    "\n",
    "\n",
    "class MarketAnalysis(BaseModel):\n",
    "    \"\"\"Output from the market specialist.\"\"\"\n",
    "    trend_analysis: str = Field(description=\"Overall market/sector trend analysis\")\n",
    "    technical_indicators: str = Field(description=\"Key technical signals\")\n",
    "    sentiment_assessment: str = Field(description=\"Market sentiment evaluation\")\n",
    "    risk_factors: List[str] = Field(description=\"Identified risk factors\")\n",
    "    opportunities: List[str] = Field(description=\"Identified opportunities\")\n",
    "    outlook: str = Field(description=\"Short to medium term outlook\")\n",
    "\n",
    "\n",
    "# === Evaluator-Optimizer Models ===\n",
    "class InvestmentAnalysis(BaseModel):\n",
    "    \"\"\"Investment analysis generated by the analyzer.\"\"\"\n",
    "    ticker: str = Field(description=\"Stock ticker symbol\")\n",
    "    recommendation: str = Field(description=\"Buy/Hold/Sell recommendation\")\n",
    "    target_price: float = Field(description=\"Price target\")\n",
    "    investment_thesis: str = Field(description=\"Core investment thesis\")\n",
    "    key_catalysts: List[str] = Field(description=\"Key catalysts for the stock\")\n",
    "    risks: List[str] = Field(description=\"Key risks\")\n",
    "    financial_highlights: str = Field(description=\"Key financial metrics and trends\")\n",
    "    conclusion: str = Field(description=\"Summary conclusion\")\n",
    "\n",
    "\n",
    "class QualityEvaluation(BaseModel):\n",
    "    \"\"\"Evaluation of analysis quality.\"\"\"\n",
    "    overall_score: float = Field(description=\"Overall quality score 0-100\")\n",
    "    completeness_score: float = Field(description=\"Completeness 0-100\")\n",
    "    accuracy_score: float = Field(description=\"Accuracy/logic 0-100\")\n",
    "    actionability_score: float = Field(description=\"Actionability 0-100\")\n",
    "    \n",
    "    strengths: List[str] = Field(description=\"What the analysis does well\")\n",
    "    weaknesses: List[str] = Field(description=\"What needs improvement\")\n",
    "    missing_elements: List[str] = Field(description=\"Important missing information\")\n",
    "    \n",
    "    specific_feedback: str = Field(description=\"Detailed constructive feedback\")\n",
    "    improvement_suggestions: List[str] = Field(description=\"Specific suggestions to improve\")\n",
    "    \n",
    "    is_acceptable: bool = Field(description=\"Whether analysis meets quality threshold\")\n",
    "\n",
    "\n",
    "# === Agent Models ===\n",
    "class ResearchPlan(BaseModel):\n",
    "    \"\"\"Research plan generated by the agent.\"\"\"\n",
    "    ticker: str = Field(description=\"Stock ticker to research\")\n",
    "    research_steps: List[str] = Field(description=\"Ordered list of research steps\")\n",
    "    data_sources_needed: List[str] = Field(description=\"Data sources required\")\n",
    "    estimated_complexity: str = Field(description=\"low, medium, or high\")\n",
    "    key_questions: List[str] = Field(description=\"Key questions to answer\")\n",
    "\n",
    "\n",
    "class ResearchReflection(BaseModel):\n",
    "    \"\"\"Self-reflection on research quality.\"\"\"\n",
    "    completeness_score: float = Field(description=\"How complete is the research (0-100)\")\n",
    "    confidence_score: float = Field(description=\"Confidence in findings (0-100)\")\n",
    "    data_quality_score: float = Field(description=\"Quality of data sources (0-100)\")\n",
    "    \n",
    "    strengths: List[str] = Field(description=\"Strong aspects of the research\")\n",
    "    gaps: List[str] = Field(description=\"Gaps or missing information\")\n",
    "    reliability_concerns: List[str] = Field(description=\"Data reliability issues\")\n",
    "    \n",
    "    overall_assessment: str = Field(description=\"Overall quality assessment\")\n",
    "    improvement_recommendations: List[str] = Field(\n",
    "        description=\"How to improve future research\"\n",
    "    )\n",
    "\n",
    "\n",
    "class ResearchResult(BaseModel):\n",
    "    \"\"\"Final research output.\"\"\"\n",
    "    ticker: str\n",
    "    plan: ResearchPlan\n",
    "    knowledge_graph_summary: str\n",
    "    news_analysis: Optional[Dict]\n",
    "    specialized_analysis: Optional[Dict]\n",
    "    investment_analysis: Optional[Dict]\n",
    "    reflection: ResearchReflection\n",
    "    execution_time_seconds: float\n",
    "\n",
    "\n",
    "print(\"✓ Data models defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Utility Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 LLM Factory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_chat_llm(temperature: float = 0.7, model_name: str = None) -> ChatOpenAI:\n",
    "    \"\"\"Create a standard chat LLM instance.\"\"\"\n",
    "    return ChatOpenAI(\n",
    "        base_url=LMSTUDIO_URL,\n",
    "        api_key=\"dummy\",\n",
    "        model_name=model_name or LLM_NAME,\n",
    "        temperature=temperature\n",
    "    )\n",
    "\n",
    "\n",
    "def create_extraction_llm(temperature: float = 0) -> ChatOpenAI:\n",
    "    \"\"\"Create an LLM instance optimized for entity extraction.\"\"\"\n",
    "    return ChatOpenAI(\n",
    "        base_url=LMSTUDIO_URL,\n",
    "        api_key=\"dummy\",\n",
    "        model_name=ENTITY_EXTRACTOR,\n",
    "        temperature=temperature\n",
    "    )\n",
    "\n",
    "\n",
    "print(\"✓ LLM factory functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Financial Data Fetchers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FinancialDataFetcher:\n",
    "    \"\"\"Fetches financial data from various sources.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize data fetching tools.\"\"\"\n",
    "        self.yahoo_news = YahooFinanceNewsTool()\n",
    "        self.wiki = WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper())\n",
    "        self.ddgo = DuckDuckGoSearchResults()\n",
    "    \n",
    "    def fetch_alpha_vantage_quote(self, ticker: str) -> str:\n",
    "        \"\"\"Fetch stock quote from Alpha Vantage.\"\"\"\n",
    "        try:\n",
    "            url = \"https://www.alphavantage.co/query\"\n",
    "            params = {\n",
    "                \"function\": \"GLOBAL_QUOTE\",\n",
    "                \"symbol\": ticker,\n",
    "                \"apikey\": ALPHA_VANTAGE_API_KEY\n",
    "            }\n",
    "            resp = requests.get(url, params=params, timeout=15)\n",
    "            data = resp.json()\n",
    "            quote = data.get(\"Global Quote\") or {}\n",
    "            \n",
    "            if not quote:\n",
    "                return \"\"\n",
    "            \n",
    "            return f\"AlphaVantage Quote for {ticker}:\\n{json.dumps(quote, indent=2)}\"\n",
    "        except Exception as e:\n",
    "            print(f\"[Error] AlphaVantage: {e}\")\n",
    "            return \"\"\n",
    "    \n",
    "    def fetch_yahoo_news(self, ticker: str) -> str:\n",
    "        \"\"\"Fetch recent news from Yahoo Finance.\"\"\"\n",
    "        try:\n",
    "            res = self.yahoo_news.invoke(ticker)\n",
    "            if isinstance(res, str) and len(res.strip()) > 50 and \"No news found\" not in res:\n",
    "                return f\"Yahoo Finance News for {ticker}:\\n{res}\"\n",
    "        except Exception as e:\n",
    "            print(f\"[Error] YahooNewsTool: {e}\")\n",
    "        return \"\"\n",
    "    \n",
    "    def fetch_wikipedia_info(self, entity_name: str) -> str:\n",
    "        \"\"\"Fetch information from Wikipedia.\"\"\"\n",
    "        try:\n",
    "            res = self.wiki.run(entity_name)\n",
    "            if isinstance(res, str) and len(res.strip()) > 50:\n",
    "                return res[:4000]  # Limit length\n",
    "        except Exception as e:\n",
    "            print(f\"[Error] Wikipedia: {e}\")\n",
    "        return \"\"\n",
    "    \n",
    "    def fetch_duckduckgo_search(self, query: str) -> str:\n",
    "        \"\"\"Fetch search results from DuckDuckGo.\"\"\"\n",
    "        try:\n",
    "            res = self.ddgo.run(query)\n",
    "            if isinstance(res, str) and len(res.strip()) > 50:\n",
    "                return res[:4000]  # Limit length\n",
    "        except Exception as e:\n",
    "            print(f\"[Error] DuckDuckGo: {e}\")\n",
    "        return \"\"\n",
    "    \n",
    "    def fetch_entity_info(self, entity_name: str, entity_label: str) -> str:\n",
    "        \"\"\"Fetch information about an entity based on its type.\"\"\"\n",
    "        entity_label = entity_label.upper()\n",
    "        text = \"\"\n",
    "        \n",
    "        # Stock symbols and companies: get financial data + news\n",
    "        if entity_label in {\"STOCK_SYMBOL\", \"ORGANIZATION\", \"COMPANY\"}:\n",
    "            text = self.fetch_alpha_vantage_quote(entity_name)\n",
    "            text += \"\\n\\n\" + self.fetch_yahoo_news(entity_name)\n",
    "        \n",
    "        # People: get background information\n",
    "        elif entity_label == \"PERSON\":\n",
    "            text = self.fetch_wikipedia_info(entity_name)\n",
    "        \n",
    "        # Policies, government: search for recent news\n",
    "        elif entity_label in {\"POLICY\", \"GOVERNMENT\", \"EVENT\"}:\n",
    "            print(f\"[Policy Search] Searching for '{entity_name}'\")\n",
    "            query = f\"{entity_name} government budget OR regulation OR policy 2025 site:reuters.com OR site:bloomberg.com\"\n",
    "            text = self.fetch_duckduckgo_search(query)\n",
    "        \n",
    "        # Fallback: try Wikipedia/DuckDuckGo\n",
    "        if not text or text.strip() == \"\":\n",
    "            wiki_result = self.fetch_wikipedia_info(entity_name)\n",
    "            if wiki_result:\n",
    "                text = wiki_result\n",
    "            else:\n",
    "                text = self.fetch_duckduckgo_search(entity_name)\n",
    "        \n",
    "        return text.strip() or f\"No relevant data found for {entity_name}\"\n",
    "\n",
    "\n",
    "print(\"✓ FinancialDataFetcher class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Knowledge Graph Builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_relevant_entity(ent: Entity) -> bool:\n",
    "    \"\"\"Filter out irrelevant or low-quality entities.\"\"\"\n",
    "    # Check minimum length\n",
    "    if not ent.text or len(ent.text.strip()) < 2:\n",
    "        return False\n",
    "    \n",
    "    # Filter numeric values (prices, percentages)\n",
    "    if ent.text.replace('.', '', 1).replace('%', '', 1).isdigit():\n",
    "        return False\n",
    "    \n",
    "    # Check if entity type is relevant\n",
    "    if ent.label.upper() not in FINANCIAL_ENTITY_LABELS:\n",
    "        return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "\n",
    "class KnowledgeGraphBuilder:\n",
    "    \"\"\"Builds and expands a knowledge graph from financial entities.\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        llm: Optional[ChatOpenAI] = None,\n",
    "        fetcher: Optional[FinancialDataFetcher] = None\n",
    "    ):\n",
    "        \"\"\"Initialize the knowledge graph builder.\"\"\"\n",
    "        self.extraction_llm = llm or create_extraction_llm()\n",
    "        self.fetcher = fetcher or FinancialDataFetcher()\n",
    "        self.graph = nx.Graph()\n",
    "        \n",
    "        # Set up structured output for NER\n",
    "        self.ner_model = self.extraction_llm.with_structured_output(NERResponse)\n",
    "    \n",
    "    def extract_entities(self, text: str) -> List[Entity]:\n",
    "        \"\"\"Extract named entities from text using NER.\"\"\"\n",
    "        try:\n",
    "            prompt = dedent(f\"\"\"Extract named entities from the following financial text.\n",
    "\n",
    "Focus on:\n",
    "- Companies and organizations\n",
    "- People (executives, analysts)\n",
    "- Stock symbols\n",
    "- Products and services\n",
    "- Events\n",
    "- Government/policy entities\n",
    "\n",
    "Text:\n",
    "{text[:8000]}\n",
    "\n",
    "Extract all relevant entities:\"\"\")\n",
    "            \n",
    "            ner_result = self.ner_model.invoke(prompt)\n",
    "            relevant_entities = [e for e in ner_result.entities if is_relevant_entity(e)]\n",
    "            return relevant_entities\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"[Error] NER extraction failed: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def add_entity_to_graph(\n",
    "        self,\n",
    "        entity: Entity,\n",
    "        related_entities: List[Entity],\n",
    "        context: Optional[str] = None\n",
    "    ):\n",
    "        \"\"\"Add an entity and its relationships to the graph.\"\"\"\n",
    "        # Add main entity node if not exists\n",
    "        if not self.graph.has_node(entity.text):\n",
    "            self.graph.add_node(\n",
    "                entity.text,\n",
    "                label=entity.label,\n",
    "                confidence=entity.confidence\n",
    "            )\n",
    "        \n",
    "        # Add related entities and edges\n",
    "        for related in related_entities:\n",
    "            if not self.graph.has_node(related.text):\n",
    "                self.graph.add_node(\n",
    "                    related.text,\n",
    "                    label=related.label,\n",
    "                    confidence=related.confidence\n",
    "                )\n",
    "            \n",
    "            # Add edge if not exists\n",
    "            if not self.graph.has_edge(entity.text, related.text):\n",
    "                self.graph.add_edge(\n",
    "                    entity.text,\n",
    "                    related.text,\n",
    "                    relation=\"mentioned_with\",\n",
    "                    context=(context or \"\")[:400]\n",
    "                )\n",
    "    \n",
    "    def expand_from_seed(\n",
    "        self,\n",
    "        seed_entity: str,\n",
    "        seed_label: str = \"STOCK_SYMBOL\",\n",
    "        depth: int = 2,\n",
    "        throttle: float = FETCH_DELAY_SECONDS\n",
    "    ) -> nx.Graph:\n",
    "        \"\"\"Expand knowledge graph starting from a seed entity.\"\"\"\n",
    "        visited: Set[str] = set()\n",
    "        current_layer = [Entity(text=seed_entity, label=seed_label)]\n",
    "        \n",
    "        print(f\"\\n=== EXPANDING KNOWLEDGE GRAPH ===\")\n",
    "        print(f\"Seed: {seed_entity} ({seed_label})\")\n",
    "        print(f\"Depth: {depth} layers\")\n",
    "        \n",
    "        for layer in range(1, depth + 1):\n",
    "            print(f\"\\n--- Layer {layer}/{depth} ---\")\n",
    "            next_layer: List[Entity] = []\n",
    "            \n",
    "            for entity in current_layer:\n",
    "                if entity.text in visited:\n",
    "                    continue\n",
    "                \n",
    "                visited.add(entity.text)\n",
    "                print(f\"[L{layer}] Processing '{entity.text}' ({entity.label})\")\n",
    "                \n",
    "                # Fetch information about this entity\n",
    "                info = self.fetcher.fetch_entity_info(entity.text, entity.label)\n",
    "                \n",
    "                if not info.strip() or info.startswith(\"No relevant data\"):\n",
    "                    print(f\"[L{layer}] No data found for '{entity.text}'\")\n",
    "                    continue\n",
    "                \n",
    "                # Extract entities from the fetched information\n",
    "                print(f\"[L{layer}] Extracting entities from data...\")\n",
    "                discovered_entities = self.extract_entities(info)\n",
    "                print(f\"[L{layer}] Found {len(discovered_entities)} entities\")\n",
    "                \n",
    "                # Add to graph\n",
    "                self.add_entity_to_graph(entity, discovered_entities, info)\n",
    "                \n",
    "                # Add undiscovered entities to next layer\n",
    "                for discovered in discovered_entities:\n",
    "                    if discovered.text not in visited:\n",
    "                        next_layer.append(discovered)\n",
    "                \n",
    "                # Throttle to avoid rate limits\n",
    "                time.sleep(throttle)\n",
    "            \n",
    "            current_layer = next_layer\n",
    "            \n",
    "            if not current_layer:\n",
    "                print(f\"\\n[L{layer}] No new entities to explore. Stopping.\")\n",
    "                break\n",
    "        \n",
    "        print(f\"\\n=== GRAPH EXPANSION COMPLETE ===\")\n",
    "        print(f\"Total nodes: {self.graph.number_of_nodes()}\")\n",
    "        print(f\"Total edges: {self.graph.number_of_edges()}\")\n",
    "        \n",
    "        return self.graph\n",
    "    \n",
    "    def get_graph_summary(self) -> str:\n",
    "        \"\"\"Generate a text summary of the graph.\"\"\"\n",
    "        lines = [\n",
    "            f\"Knowledge Graph Summary:\",\n",
    "            f\"  Nodes: {self.graph.number_of_nodes()}\",\n",
    "            f\"  Edges: {self.graph.number_of_edges()}\",\n",
    "            f\"\\nEntities by Type:\"\n",
    "        ]\n",
    "        \n",
    "        # Count entities by type\n",
    "        label_counts: Dict[str, int] = {}\n",
    "        for node, data in self.graph.nodes(data=True):\n",
    "            label = data.get('label', 'UNKNOWN')\n",
    "            label_counts[label] = label_counts.get(label, 0) + 1\n",
    "        \n",
    "        for label, count in sorted(label_counts.items(), key=lambda x: x[1], reverse=True):\n",
    "            lines.append(f\"  {label}: {count}\")\n",
    "        \n",
    "        lines.append(f\"\\nTop Connected Entities:\")\n",
    "        # Get nodes sorted by degree (number of connections)\n",
    "        node_degrees = [(node, self.graph.degree(node)) for node in self.graph.nodes()]\n",
    "        node_degrees.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        for node, degree in node_degrees[:10]:\n",
    "            label = self.graph.nodes[node].get('label', 'UNKNOWN')\n",
    "            lines.append(f\"  {node} ({label}): {degree} connections\")\n",
    "        \n",
    "        return \"\\n\".join(lines)\n",
    "\n",
    "\n",
    "print(\"✓ KnowledgeGraphBuilder class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Workflow Pattern 1: Prompt Chaining\n",
    "\n",
    "Sequential pipeline: Ingest → Preprocess → Classify → Extract → Summarize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PromptChainWorkflow:\n",
    "    \"\"\"Implements the Prompt Chaining workflow pattern for news analysis.\"\"\"\n",
    "    \n",
    "    def __init__(self, llm: Optional[ChatOpenAI] = None):\n",
    "        \"\"\"Initialize the workflow with an LLM instance.\"\"\"\n",
    "        self.llm = llm or create_chat_llm(temperature=0.3)\n",
    "    \n",
    "    def step1_ingest(self, raw_text: str, source: str = \"Unknown\") -> NewsArticle:\n",
    "        \"\"\"Step 1: Ingest raw news data.\"\"\"\n",
    "        lines = raw_text.strip().split('\\n')\n",
    "        title = lines[0] if lines else \"Untitled\"\n",
    "        content = '\\n'.join(lines[1:]) if len(lines) > 1 else raw_text\n",
    "        \n",
    "        return NewsArticle(\n",
    "            title=title,\n",
    "            content=content,\n",
    "            source=source\n",
    "        )\n",
    "    \n",
    "    def step2_preprocess(self, article: NewsArticle) -> PreprocessedNews:\n",
    "        \"\"\"Step 2: Clean and preprocess the news article.\"\"\"\n",
    "        prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", \"You are a text preprocessing expert. Clean and normalize the following news article. \"\n",
    "                      \"Remove ads, boilerplate, redundant info. Keep only substantive content.\"),\n",
    "            (\"user\", \"Title: {title}\\n\\nContent: {content}\")\n",
    "        ])\n",
    "        \n",
    "        chain = prompt | self.llm\n",
    "        response = chain.invoke({\n",
    "            \"title\": article.title,\n",
    "            \"content\": article.content\n",
    "        })\n",
    "        \n",
    "        cleaned_content = response.content\n",
    "        word_count = len(cleaned_content.split())\n",
    "        contains_financials = any(term in cleaned_content.lower()\n",
    "                                 for term in ['revenue', 'earnings', 'eps', 'profit', 'loss', 'quarter'])\n",
    "        \n",
    "        return PreprocessedNews(\n",
    "            title=article.title,\n",
    "            cleaned_content=cleaned_content,\n",
    "            source=article.source,\n",
    "            word_count=word_count,\n",
    "            contains_financials=contains_financials\n",
    "        )\n",
    "    \n",
    "    def step3_classify(self, preprocessed: PreprocessedNews) -> NewsClassification:\n",
    "        \"\"\"Step 3: Classify the news article by category and sentiment.\"\"\"\n",
    "        structured_llm = self.llm.with_structured_output(NewsClassification)\n",
    "        \n",
    "        prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", \"\"\"You are a financial news classifier. Analyze the article and classify it.\n",
    "\n",
    "Categories:\n",
    "- earnings: Quarterly/annual earnings reports\n",
    "- market_analysis: Market trends, sector analysis\n",
    "- policy: Government policy, regulation, central bank\n",
    "- merger_acquisition: M&A, partnerships, deals\n",
    "- general: Other business news\n",
    "\n",
    "Sentiment: positive, negative, or neutral\n",
    "Relevance: How relevant to investors (0-1)\n",
    "Confidence: Your confidence in this classification (0-1)\"\"\"),\n",
    "            (\"user\", \"Title: {title}\\n\\nContent: {content}\")\n",
    "        ])\n",
    "        \n",
    "        chain = prompt | structured_llm\n",
    "        return chain.invoke({\n",
    "            \"title\": preprocessed.title,\n",
    "            \"content\": preprocessed.cleaned_content\n",
    "        })\n",
    "    \n",
    "    def step4_extract(self, preprocessed: PreprocessedNews) -> ExtractedEntities:\n",
    "        \"\"\"Step 4: Extract key entities and facts from the article.\"\"\"\n",
    "        structured_llm = self.llm.with_structured_output(ExtractedEntities)\n",
    "        \n",
    "        prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", \"\"\"Extract key entities from this financial news article.\n",
    "\n",
    "Focus on:\n",
    "- Company names and stock symbols\n",
    "- Key executives, analysts, officials\n",
    "- Financial metrics (revenue, EPS, growth %, margins, etc.)\n",
    "- Important events or announcements\n",
    "\n",
    "Be precise and extract only what's explicitly mentioned.\"\"\"),\n",
    "            (\"user\", \"{content}\")\n",
    "        ])\n",
    "        \n",
    "        chain = prompt | structured_llm\n",
    "        return chain.invoke({\"content\": preprocessed.cleaned_content})\n",
    "    \n",
    "    def step5_summarize(\n",
    "        self,\n",
    "        preprocessed: PreprocessedNews,\n",
    "        classification: NewsClassification,\n",
    "        entities: ExtractedEntities\n",
    "    ) -> NewsSummary:\n",
    "        \"\"\"Step 5: Generate final summary with investment implications.\"\"\"\n",
    "        structured_llm = self.llm.with_structured_output(NewsSummary)\n",
    "        \n",
    "        prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", \"\"\"You are a financial analyst. Create a concise summary for investors.\n",
    "\n",
    "Include:\n",
    "- Executive summary (2-3 sentences)\n",
    "- Key points as bullet list\n",
    "- Investment implications (what this means for investors)\n",
    "\n",
    "Be clear, actionable, and focused on financial impact.\"\"\"),\n",
    "            (\"user\", \"\"\"Article: {title}\n",
    "\n",
    "Content: {content}\n",
    "\n",
    "Category: {category}\n",
    "Sentiment: {sentiment}\n",
    "\n",
    "Extracted Entities:\n",
    "Companies: {companies}\n",
    "People: {people}\n",
    "Financial Metrics: {metrics}\n",
    "Key Events: {events}\n",
    "\n",
    "Create a comprehensive summary.\"\"\")\n",
    "        ])\n",
    "        \n",
    "        chain = prompt | structured_llm\n",
    "        result = chain.invoke({\n",
    "            \"title\": preprocessed.title,\n",
    "            \"content\": preprocessed.cleaned_content,\n",
    "            \"category\": classification.category,\n",
    "            \"sentiment\": classification.sentiment,\n",
    "            \"companies\": \", \".join(entities.companies) if entities.companies else \"None\",\n",
    "            \"people\": \", \".join(entities.people) if entities.people else \"None\",\n",
    "            \"metrics\": \", \".join(entities.financial_metrics) if entities.financial_metrics else \"None\",\n",
    "            \"events\": \", \".join(entities.key_events) if entities.key_events else \"None\"\n",
    "        })\n",
    "        \n",
    "        # Attach entities and classification to summary\n",
    "        result.entities = entities\n",
    "        result.classification = classification\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def run(self, raw_text: str, source: str = \"Unknown\") -> NewsSummary:\n",
    "        \"\"\"Execute the complete prompt chaining workflow.\"\"\"\n",
    "        print(\"\\n=== PROMPT CHAINING WORKFLOW ===\")\n",
    "        \n",
    "        # Step 1: Ingest\n",
    "        print(\"\\n[1/5] Ingesting raw news...\")\n",
    "        article = self.step1_ingest(raw_text, source)\n",
    "        print(f\"✓ Ingested: {article.title[:50]}...\")\n",
    "        \n",
    "        # Step 2: Preprocess\n",
    "        print(\"\\n[2/5] Preprocessing content...\")\n",
    "        preprocessed = self.step2_preprocess(article)\n",
    "        print(f\"✓ Preprocessed: {preprocessed.word_count} words, \"\n",
    "              f\"Contains financials: {preprocessed.contains_financials}\")\n",
    "        \n",
    "        # Step 3: Classify\n",
    "        print(\"\\n[3/5] Classifying news...\")\n",
    "        classification = self.step3_classify(preprocessed)\n",
    "        print(f\"✓ Classified: {classification.category} | {classification.sentiment} \"\n",
    "              f\"(confidence: {classification.confidence:.2f})\")\n",
    "        \n",
    "        # Step 4: Extract\n",
    "        print(\"\\n[4/5] Extracting entities...\")\n",
    "        entities = self.step4_extract(preprocessed)\n",
    "        print(f\"✓ Extracted: {len(entities.companies)} companies, \"\n",
    "              f\"{len(entities.financial_metrics)} metrics\")\n",
    "        \n",
    "        # Step 5: Summarize\n",
    "        print(\"\\n[5/5] Generating summary...\")\n",
    "        summary = self.step5_summarize(preprocessed, classification, entities)\n",
    "        print(f\"✓ Summary complete: {len(summary.key_points)} key points\")\n",
    "        \n",
    "        print(\"\\n=== WORKFLOW COMPLETE ===\\n\")\n",
    "        return summary\n",
    "\n",
    "\n",
    "print(\"✓ PromptChainWorkflow class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo: Prompt Chaining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample news for demonstration\n",
    "sample_news = \"\"\"\n",
    "Microsoft Reports Strong Q4 Earnings, Cloud Revenue Surges 25%\n",
    "\n",
    "Microsoft Corporation (MSFT) announced its fiscal Q4 2024 results today, beating analyst\n",
    "expectations with revenue of $61.9 billion, up 15% year-over-year. CEO Satya Nadella\n",
    "highlighted the company's AI investments, noting that Azure cloud services grew 25%\n",
    "driven by demand for AI infrastructure.\n",
    "\n",
    "Earnings per share came in at $2.95, compared to consensus estimates of $2.85. The\n",
    "Intelligent Cloud segment generated $28.5 billion in revenue, while Productivity and\n",
    "Business Processes reached $19.6 billion.\n",
    "\n",
    "\"We are seeing unprecedented demand for AI capabilities across our cloud platform,\"\n",
    "Nadella stated during the earnings call. The company also announced a 10% increase\n",
    "in its quarterly dividend to $0.75 per share.\n",
    "\n",
    "Shares rose 4% in after-hours trading following the announcement.\n",
    "\"\"\"\n",
    "\n",
    "# Run the workflow\n",
    "workflow = PromptChainWorkflow()\n",
    "summary = workflow.run(sample_news, source=\"Company Press Release\")\n",
    "\n",
    "# Display results\n",
    "print(\"\\n=== FINAL SUMMARY ===\")\n",
    "print(f\"\\nExecutive Summary:\\n{summary.executive_summary}\")\n",
    "print(f\"\\nKey Points:\")\n",
    "for i, point in enumerate(summary.key_points, 1):\n",
    "    print(f\"  {i}. {point}\")\n",
    "print(f\"\\nInvestment Implications:\\n{summary.investment_implications}\")\n",
    "print(f\"\\nCategory: {summary.classification.category}\")\n",
    "print(f\"Sentiment: {summary.classification.sentiment}\")\n",
    "print(f\"Companies: {', '.join(summary.entities.companies)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Workflow Pattern 2: Routing\n",
    "\n",
    "Routes content to specialized analyst agents based on content type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContentRouter:\n",
    "    \"\"\"Routes incoming content to specialized analyst agents.\"\"\"\n",
    "    \n",
    "    def __init__(self, llm: Optional[ChatOpenAI] = None):\n",
    "        \"\"\"Initialize the router with an LLM.\"\"\"\n",
    "        self.llm = llm or create_chat_llm(temperature=0.1)\n",
    "    \n",
    "    def route(self, content: str, title: str = \"\") -> RoutingDecision:\n",
    "        \"\"\"Determine which specialist should analyze this content.\"\"\"\n",
    "        structured_llm = self.llm.with_structured_output(RoutingDecision)\n",
    "        \n",
    "        prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", \"\"\"You are a content router for financial analysis. Route content to specialists:\n",
    "\n",
    "**earnings**: Quarterly/annual earnings reports, financial statements, revenue/profit data\n",
    "**news**: Breaking news, events, announcements, M&A, policy changes, management changes\n",
    "**market**: Market trends, sector analysis, technical analysis, broad market commentary\n",
    "\n",
    "Choose the MOST appropriate specialist based on the primary focus of the content.\n",
    "Provide reasoning for your choice.\"\"\"),\n",
    "            (\"user\", \"Title: {title}\\n\\nContent: {content}\\n\\nWhich specialist should analyze this?\")\n",
    "        ])\n",
    "        \n",
    "        chain = prompt | structured_llm\n",
    "        return chain.invoke({\"title\": title, \"content\": content})\n",
    "\n",
    "\n",
    "class EarningsAnalyst:\n",
    "    \"\"\"Specialist agent for earnings analysis.\"\"\"\n",
    "    \n",
    "    def __init__(self, llm: Optional[ChatOpenAI] = None):\n",
    "        self.llm = llm or create_chat_llm(temperature=0.2)\n",
    "    \n",
    "    def analyze(self, content: str) -> EarningsAnalysis:\n",
    "        \"\"\"Perform deep earnings analysis.\"\"\"\n",
    "        structured_llm = self.llm.with_structured_output(EarningsAnalysis)\n",
    "        \n",
    "        prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", \"\"\"You are an expert earnings analyst. Analyze financial results thoroughly.\n",
    "\n",
    "Focus on:\n",
    "- Revenue growth and trends\n",
    "- Profitability (margins, EPS, operating income)\n",
    "- YoY and QoQ comparisons\n",
    "- Forward guidance quality\n",
    "- Key performance metrics\n",
    "- Investment implications\n",
    "\n",
    "Provide actionable recommendations for investors.\"\"\"),\n",
    "            (\"user\", \"Analyze this earnings content:\\n\\n{content}\")\n",
    "        ])\n",
    "        \n",
    "        chain = prompt | structured_llm\n",
    "        return chain.invoke({\"content\": content})\n",
    "\n",
    "\n",
    "class NewsAnalyst:\n",
    "    \"\"\"Specialist agent for news analysis.\"\"\"\n",
    "    \n",
    "    def __init__(self, llm: Optional[ChatOpenAI] = None):\n",
    "        self.llm = llm or create_chat_llm(temperature=0.3)\n",
    "    \n",
    "    def analyze(self, content: str) -> NewsAnalysis:\n",
    "        \"\"\"Perform news impact analysis.\"\"\"\n",
    "        structured_llm = self.llm.with_structured_output(NewsAnalysis)\n",
    "        \n",
    "        prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", \"\"\"You are an expert news analyst specializing in financial news impact.\n",
    "\n",
    "Analyze:\n",
    "- What happened and why it matters\n",
    "- Potential market impact (stocks, sectors affected)\n",
    "- Stakeholder implications (companies, investors, regulators)\n",
    "- Timeline of developments\n",
    "- Source credibility\n",
    "- Actionable insights for investors\n",
    "\n",
    "Be objective and focus on investment implications.\"\"\"),\n",
    "            (\"user\", \"Analyze this news:\\n\\n{content}\")\n",
    "        ])\n",
    "        \n",
    "        chain = prompt | structured_llm\n",
    "        return chain.invoke({\"content\": content})\n",
    "\n",
    "\n",
    "class MarketAnalyst:\n",
    "    \"\"\"Specialist agent for market analysis.\"\"\"\n",
    "    \n",
    "    def __init__(self, llm: Optional[ChatOpenAI] = None):\n",
    "        self.llm = llm or create_chat_llm(temperature=0.3)\n",
    "    \n",
    "    def analyze(self, content: str) -> MarketAnalysis:\n",
    "        \"\"\"Perform market trend analysis.\"\"\"\n",
    "        structured_llm = self.llm.with_structured_output(MarketAnalysis)\n",
    "        \n",
    "        prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", \"\"\"You are an expert market analyst. Analyze market trends and conditions.\n",
    "\n",
    "Evaluate:\n",
    "- Overall market/sector trends\n",
    "- Technical indicators and signals\n",
    "- Market sentiment (bullish/bearish/neutral)\n",
    "- Risk factors to watch\n",
    "- Investment opportunities\n",
    "- Short to medium term outlook\n",
    "\n",
    "Provide balanced, data-driven analysis.\"\"\"),\n",
    "            (\"user\", \"Analyze this market content:\\n\\n{content}\")\n",
    "        ])\n",
    "        \n",
    "        chain = prompt | structured_llm\n",
    "        return chain.invoke({\"content\": content})\n",
    "\n",
    "\n",
    "class RoutingWorkflow:\n",
    "    \"\"\"Complete routing workflow that coordinates content routing and analysis.\"\"\"\n",
    "    \n",
    "    def __init__(self, llm: Optional[ChatOpenAI] = None):\n",
    "        \"\"\"Initialize the routing workflow with all specialists.\"\"\"\n",
    "        self.router = ContentRouter(llm)\n",
    "        self.earnings_analyst = EarningsAnalyst(llm)\n",
    "        self.news_analyst = NewsAnalyst(llm)\n",
    "        self.market_analyst = MarketAnalyst(llm)\n",
    "    \n",
    "    def process(self, content: str, title: str = \"\") -> Dict:\n",
    "        \"\"\"Route content to appropriate specialist and return analysis.\"\"\"\n",
    "        print(\"\\n=== ROUTING WORKFLOW ===\")\n",
    "        \n",
    "        # Step 1: Route the content\n",
    "        print(\"\\n[1/2] Routing content to specialist...\")\n",
    "        routing_decision = self.router.route(content, title)\n",
    "        print(f\"✓ Routed to: {routing_decision.route.upper()} analyst \"\n",
    "              f\"(confidence: {routing_decision.confidence:.2f})\")\n",
    "        print(f\"  Reasoning: {routing_decision.reasoning}\")\n",
    "        \n",
    "        # Step 2: Analyze with appropriate specialist\n",
    "        print(f\"\\n[2/2] {routing_decision.route.upper()} analyst processing...\")\n",
    "        \n",
    "        if routing_decision.route == \"earnings\":\n",
    "            analysis = self.earnings_analyst.analyze(content)\n",
    "            analysis_type = \"Earnings Analysis\"\n",
    "        elif routing_decision.route == \"news\":\n",
    "            analysis = self.news_analyst.analyze(content)\n",
    "            analysis_type = \"News Analysis\"\n",
    "        else:  # market\n",
    "            analysis = self.market_analyst.analyze(content)\n",
    "            analysis_type = \"Market Analysis\"\n",
    "        \n",
    "        print(f\"✓ {analysis_type} complete\")\n",
    "        print(\"\\n=== WORKFLOW COMPLETE ===\\n\")\n",
    "        \n",
    "        return {\n",
    "            \"routing_decision\": routing_decision,\n",
    "            \"analysis_type\": analysis_type,\n",
    "            \"analysis\": analysis\n",
    "        }\n",
    "    \n",
    "    def batch_process(self, content_items: List[Dict[str, str]]) -> List[Dict]:\n",
    "        \"\"\"Process multiple content items, routing each appropriately.\"\"\"\n",
    "        results = []\n",
    "        for i, item in enumerate(content_items, 1):\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(f\"Processing item {i}/{len(content_items)}\")\n",
    "            print(f\"{'='*60}\")\n",
    "            \n",
    "            result = self.process(\n",
    "                content=item.get(\"content\", \"\"),\n",
    "                title=item.get(\"title\", \"\")\n",
    "            )\n",
    "            results.append(result)\n",
    "        \n",
    "        return results\n",
    "\n",
    "\n",
    "print(\"✓ Routing workflow classes defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo: Routing Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "routing_workflow = RoutingWorkflow()\n",
    "\n",
    "# Sample content of different types\n",
    "test_content = [\n",
    "    {\n",
    "        \"title\": \"Apple Reports Q1 Earnings Beat\",\n",
    "        \"content\": \"\"\"\n",
    "        Apple Inc. reported fiscal Q1 results with revenue of $119.6 billion, up 2% YoY,\n",
    "        beating estimates of $118.3 billion. iPhone revenue reached $69.7 billion, up 6%.\n",
    "        Services revenue hit a record $23.1 billion. EPS came in at $2.18 vs $2.10 expected.\n",
    "        Gross margin improved to 45.9% from 43.0% last year. CEO Tim Cook noted strong\n",
    "        demand in emerging markets. The company announced a 4% dividend increase.\n",
    "        \"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"title\": \"Tech Sector Shows Bullish Momentum\",\n",
    "        \"content\": \"\"\"\n",
    "        Technology stocks continued their uptrend with the Nasdaq composite gaining 15%\n",
    "        YTD. Strong momentum in AI-related names is driving the rally. The tech sector\n",
    "        RSI stands at 68, approaching overbought territory. Volume has been above average,\n",
    "        confirming the trend. Support levels have held at the 50-day moving average.\n",
    "        Sector rotation shows investors favoring growth over value.\n",
    "        \"\"\"\n",
    "    }\n",
    "]\n",
    "\n",
    "results = routing_workflow.batch_process(test_content)\n",
    "\n",
    "# Display summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ROUTING SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "for i, result in enumerate(results, 1):\n",
    "    print(f\"\\nContent {i}: {test_content[i-1]['title']}\")\n",
    "    print(f\"  → Routed to: {result['routing_decision'].route}\")\n",
    "    print(f\"  → Analysis type: {result['analysis_type']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Workflow Pattern 3: Evaluator-Optimizer\n",
    "\n",
    "Generate → Evaluate → Refine → Re-evaluate (Iterative Self-Improvement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InvestmentAnalyzer:\n",
    "    \"\"\"Generates initial investment analysis (the generator).\"\"\"\n",
    "    \n",
    "    def __init__(self, llm: Optional[ChatOpenAI] = None):\n",
    "        self.llm = llm or create_chat_llm(temperature=0.4)\n",
    "    \n",
    "    def generate(self, ticker: str, company_data: str) -> InvestmentAnalysis:\n",
    "        \"\"\"Generate initial investment analysis.\"\"\"\n",
    "        structured_llm = self.llm.with_structured_output(InvestmentAnalysis)\n",
    "        \n",
    "        prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", \"\"\"You are an investment analyst. Generate a comprehensive stock analysis.\n",
    "\n",
    "Include:\n",
    "- Clear Buy/Hold/Sell recommendation\n",
    "- Price target with justification\n",
    "- Investment thesis (why this stock?)\n",
    "- Key catalysts (what will drive the stock?)\n",
    "- Risks (what could go wrong?)\n",
    "- Financial highlights (metrics, trends)\n",
    "- Conclusion\n",
    "\n",
    "Base your analysis on the provided data. Be specific and actionable.\"\"\"),\n",
    "            (\"user\", \"Ticker: {ticker}\\n\\nCompany Data:\\n{company_data}\\n\\nProvide your analysis:\")\n",
    "        ])\n",
    "        \n",
    "        chain = prompt | structured_llm\n",
    "        return chain.invoke({\"ticker\": ticker, \"company_data\": company_data})\n",
    "\n",
    "\n",
    "class AnalysisEvaluator:\n",
    "    \"\"\"Evaluates quality of investment analysis (the evaluator).\"\"\"\n",
    "    \n",
    "    def __init__(self, llm: Optional[ChatOpenAI] = None):\n",
    "        self.llm = llm or create_chat_llm(temperature=0.2)\n",
    "    \n",
    "    def evaluate(self, analysis: InvestmentAnalysis, original_data: str) -> QualityEvaluation:\n",
    "        \"\"\"Evaluate the quality of an investment analysis.\"\"\"\n",
    "        structured_llm = self.llm.with_structured_output(QualityEvaluation)\n",
    "        \n",
    "        prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", \"\"\"You are a senior investment analyst reviewing a junior analyst's work.\n",
    "Evaluate the analysis critically but constructively.\n",
    "\n",
    "Score each dimension 0-100:\n",
    "- Completeness: Are all key aspects covered?\n",
    "- Accuracy: Is the logic sound? Are claims supported?\n",
    "- Actionability: Can an investor act on this?\n",
    "\n",
    "Identify:\n",
    "- Strengths (what's done well)\n",
    "- Weaknesses (what's lacking)\n",
    "- Missing elements (what's not covered)\n",
    "- Specific improvements needed\n",
    "\n",
    "An analysis is acceptable (is_acceptable=true) only if overall_score >= 75.\n",
    "Be thorough and specific in your feedback.\"\"\"),\n",
    "            (\"user\", \"\"\"Original Data:\n",
    "{original_data}\n",
    "\n",
    "Analysis to Evaluate:\n",
    "Ticker: {ticker}\n",
    "Recommendation: {recommendation}\n",
    "Target Price: ${target_price}\n",
    "\n",
    "Investment Thesis:\n",
    "{investment_thesis}\n",
    "\n",
    "Key Catalysts:\n",
    "{catalysts}\n",
    "\n",
    "Risks:\n",
    "{risks}\n",
    "\n",
    "Financial Highlights:\n",
    "{financials}\n",
    "\n",
    "Conclusion:\n",
    "{conclusion}\n",
    "\n",
    "Provide your evaluation:\"\"\")\n",
    "        ])\n",
    "        \n",
    "        chain = prompt | structured_llm\n",
    "        return chain.invoke({\n",
    "            \"original_data\": original_data,\n",
    "            \"ticker\": analysis.ticker,\n",
    "            \"recommendation\": analysis.recommendation,\n",
    "            \"target_price\": analysis.target_price,\n",
    "            \"investment_thesis\": analysis.investment_thesis,\n",
    "            \"catalysts\": \"\\n\".join(f\"- {c}\" for c in analysis.key_catalysts),\n",
    "            \"risks\": \"\\n\".join(f\"- {r}\" for r in analysis.risks),\n",
    "            \"financials\": analysis.financial_highlights,\n",
    "            \"conclusion\": analysis.conclusion\n",
    "        })\n",
    "\n",
    "\n",
    "class AnalysisOptimizer:\n",
    "    \"\"\"Refines analysis based on evaluation feedback (the optimizer).\"\"\"\n",
    "    \n",
    "    def __init__(self, llm: Optional[ChatOpenAI] = None):\n",
    "        self.llm = llm or create_chat_llm(temperature=0.3)\n",
    "    \n",
    "    def refine(\n",
    "        self,\n",
    "        original_analysis: InvestmentAnalysis,\n",
    "        evaluation: QualityEvaluation,\n",
    "        company_data: str\n",
    "    ) -> InvestmentAnalysis:\n",
    "        \"\"\"Refine analysis based on evaluation feedback.\"\"\"\n",
    "        structured_llm = self.llm.with_structured_output(InvestmentAnalysis)\n",
    "        \n",
    "        prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", \"\"\"You are refining an investment analysis based on expert feedback.\n",
    "\n",
    "Your task:\n",
    "1. Address all weaknesses identified\n",
    "2. Add missing elements\n",
    "3. Implement improvement suggestions\n",
    "4. Expand areas that need more detail\n",
    "5. Clarify ambiguous points\n",
    "\n",
    "Keep the strengths, fix the weaknesses. Make the analysis significantly better.\"\"\"),\n",
    "            (\"user\", \"\"\"Original Analysis:\n",
    "Ticker: {ticker}\n",
    "Recommendation: {recommendation}\n",
    "Target Price: ${target_price}\n",
    "Investment Thesis: {investment_thesis}\n",
    "\n",
    "Evaluation Feedback:\n",
    "Overall Score: {overall_score}/100\n",
    "Strengths: {strengths}\n",
    "Weaknesses: {weaknesses}\n",
    "Missing Elements: {missing_elements}\n",
    "\n",
    "Specific Feedback:\n",
    "{specific_feedback}\n",
    "\n",
    "Improvement Suggestions:\n",
    "{improvements}\n",
    "\n",
    "Company Data (for reference):\n",
    "{company_data}\n",
    "\n",
    "Provide a refined, improved analysis:\"\"\")\n",
    "        ])\n",
    "        \n",
    "        chain = prompt | structured_llm\n",
    "        return chain.invoke({\n",
    "            \"ticker\": original_analysis.ticker,\n",
    "            \"recommendation\": original_analysis.recommendation,\n",
    "            \"target_price\": original_analysis.target_price,\n",
    "            \"investment_thesis\": original_analysis.investment_thesis,\n",
    "            \"overall_score\": evaluation.overall_score,\n",
    "            \"strengths\": \", \".join(evaluation.strengths),\n",
    "            \"weaknesses\": \", \".join(evaluation.weaknesses),\n",
    "            \"missing_elements\": \", \".join(evaluation.missing_elements),\n",
    "            \"specific_feedback\": evaluation.specific_feedback,\n",
    "            \"improvements\": \"\\n\".join(f\"- {s}\" for s in evaluation.improvement_suggestions),\n",
    "            \"company_data\": company_data\n",
    "        })\n",
    "\n",
    "\n",
    "class EvaluatorOptimizerWorkflow:\n",
    "    \"\"\"Complete evaluator-optimizer workflow with iterative refinement.\"\"\"\n",
    "    \n",
    "    def __init__(self, llm: Optional[ChatOpenAI] = None, max_iterations: int = 3):\n",
    "        self.analyzer = InvestmentAnalyzer(llm)\n",
    "        self.evaluator = AnalysisEvaluator(llm)\n",
    "        self.optimizer = AnalysisOptimizer(llm)\n",
    "        self.max_iterations = max_iterations\n",
    "        self.quality_threshold = 75.0\n",
    "    \n",
    "    def run(self, ticker: str, company_data: str, verbose: bool = True) -> Dict:\n",
    "        \"\"\"Execute the complete evaluator-optimizer workflow.\"\"\"\n",
    "        if verbose:\n",
    "            print(\"\\n=== EVALUATOR-OPTIMIZER WORKFLOW ===\")\n",
    "        \n",
    "        iteration_history = []\n",
    "        current_analysis = None\n",
    "        current_evaluation = None\n",
    "        \n",
    "        for iteration in range(1, self.max_iterations + 1):\n",
    "            if verbose:\n",
    "                print(f\"\\n--- Iteration {iteration}/{self.max_iterations} ---\")\n",
    "            \n",
    "            # Generate or refine analysis\n",
    "            if iteration == 1:\n",
    "                if verbose:\n",
    "                    print(\"\\n[1] Generating initial analysis...\")\n",
    "                current_analysis = self.analyzer.generate(ticker, company_data)\n",
    "                if verbose:\n",
    "                    print(f\"✓ Initial analysis complete: {current_analysis.recommendation} \"\n",
    "                          f\"recommendation, ${current_analysis.target_price} target\")\n",
    "            else:\n",
    "                if verbose:\n",
    "                    print(f\"\\n[1] Refining analysis based on feedback...\")\n",
    "                current_analysis = self.optimizer.refine(\n",
    "                    current_analysis,\n",
    "                    current_evaluation,\n",
    "                    company_data\n",
    "                )\n",
    "                if verbose:\n",
    "                    print(f\"✓ Refined analysis: {current_analysis.recommendation} \"\n",
    "                          f\"recommendation, ${current_analysis.target_price} target\")\n",
    "            \n",
    "            # Evaluate\n",
    "            if verbose:\n",
    "                print(f\"\\n[2] Evaluating analysis quality...\")\n",
    "            current_evaluation = self.evaluator.evaluate(current_analysis, company_data)\n",
    "            \n",
    "            if verbose:\n",
    "                print(f\"✓ Quality Score: {current_evaluation.overall_score:.1f}/100\")\n",
    "                print(f\"  - Completeness: {current_evaluation.completeness_score:.1f}\")\n",
    "                print(f\"  - Accuracy: {current_evaluation.accuracy_score:.1f}\")\n",
    "                print(f\"  - Actionability: {current_evaluation.actionability_score:.1f}\")\n",
    "                print(f\"  - Acceptable: {current_evaluation.is_acceptable}\")\n",
    "            \n",
    "            # Store iteration\n",
    "            iteration_history.append({\n",
    "                \"iteration\": iteration,\n",
    "                \"analysis\": current_analysis,\n",
    "                \"evaluation\": current_evaluation\n",
    "            })\n",
    "            \n",
    "            # Check if we've met quality threshold\n",
    "            if current_evaluation.overall_score >= self.quality_threshold:\n",
    "                if verbose:\n",
    "                    print(f\"\\n✓ Quality threshold met ({self.quality_threshold})! \"\n",
    "                          f\"Analysis accepted.\")\n",
    "                break\n",
    "            \n",
    "            if iteration < self.max_iterations:\n",
    "                if verbose:\n",
    "                    print(f\"\\n  Quality below threshold. Refining...\")\n",
    "            else:\n",
    "                if verbose:\n",
    "                    print(f\"\\n⚠ Max iterations reached. Using best available analysis.\")\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"\\n=== WORKFLOW COMPLETE ===\\n\")\n",
    "        \n",
    "        return {\n",
    "            \"final_analysis\": current_analysis,\n",
    "            \"final_evaluation\": current_evaluation,\n",
    "            \"iteration_history\": iteration_history,\n",
    "            \"iterations_performed\": len(iteration_history),\n",
    "            \"quality_threshold_met\": current_evaluation.overall_score >= self.quality_threshold\n",
    "        }\n",
    "\n",
    "\n",
    "print(\"✓ Evaluator-Optimizer workflow classes defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo: Evaluator-Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data = \"\"\"\n",
    "Tesla Inc. (TSLA)\n",
    "\n",
    "Recent Financial Performance:\n",
    "- Q3 2024 Revenue: $25.2B (+8% YoY)\n",
    "- Automotive Revenue: $20.0B\n",
    "- Energy & Services: $5.2B (+20% YoY)\n",
    "- Net Income: $1.85B\n",
    "- EPS: $0.53 (beat estimates of $0.46)\n",
    "- Operating Margin: 7.6% (down from 11.0% last year)\n",
    "- Free Cash Flow: $1.3B\n",
    "\n",
    "Recent Developments:\n",
    "- Price cuts across major models to stimulate demand\n",
    "- Cybertruck production ramping up, now at 1,000 units/week\n",
    "- Energy storage deployments up 73% YoY\n",
    "- FSD (Full Self-Driving) subscription revenue growing\n",
    "- Opening Gigafactory in Mexico delayed to 2025\n",
    "\n",
    "Market Metrics:\n",
    "- Current Price: $242\n",
    "- 52-week range: $152 - $299\n",
    "- P/E Ratio: 75\n",
    "- Market Cap: $765B\n",
    "\n",
    "Industry Context:\n",
    "- EV market growth slowing (was 40% YoY, now 15%)\n",
    "- Increased competition from legacy automakers\n",
    "- Federal EV tax credits under review\n",
    "\"\"\"\n",
    "\n",
    "eo_workflow = EvaluatorOptimizerWorkflow(max_iterations=3)\n",
    "result = eo_workflow.run(\"TSLA\", sample_data, verbose=True)\n",
    "\n",
    "# Display final results\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL ANALYSIS RESULTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "final = result[\"final_analysis\"]\n",
    "eval_final = result[\"final_evaluation\"]\n",
    "\n",
    "print(f\"\\nTicker: {final.ticker}\")\n",
    "print(f\"Recommendation: {final.recommendation}\")\n",
    "print(f\"Target Price: ${final.target_price}\")\n",
    "print(f\"\\nInvestment Thesis:\\n{final.investment_thesis}\")\n",
    "print(f\"\\nKey Catalysts:\")\n",
    "for catalyst in final.key_catalysts:\n",
    "    print(f\"  • {catalyst}\")\n",
    "print(f\"\\nKey Risks:\")\n",
    "for risk in final.risks:\n",
    "    print(f\"  • {risk}\")\n",
    "\n",
    "print(f\"\\n{'-'*60}\")\n",
    "print(\"QUALITY METRICS\")\n",
    "print(f\"{'-'*60}\")\n",
    "print(f\"Final Score: {eval_final.overall_score:.1f}/100\")\n",
    "print(f\"Iterations: {result['iterations_performed']}\")\n",
    "print(f\"Threshold Met: {result['quality_threshold_met']}\")\n",
    "\n",
    "print(f\"\\nScore Progression:\")\n",
    "for item in result[\"iteration_history\"]:\n",
    "    score = item[\"evaluation\"].overall_score\n",
    "    print(f\"  Iteration {item['iteration']}: {score:.1f}/100\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Autonomous Research Agent\n",
    "\n",
    "Integrates all three workflow patterns into a complete autonomous agent that:\n",
    "1. Plans research steps\n",
    "2. Dynamically uses tools\n",
    "3. Self-reflects on quality\n",
    "4. Learns across runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutonomousResearchAgent:\n",
    "    \"\"\"Autonomous agent that researches stocks with planning, tool use, and self-reflection.\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        llm: Optional[ChatOpenAI] = None,\n",
    "        memory_file: str = MEMORY_FILE\n",
    "    ):\n",
    "        self.llm = llm or create_chat_llm(temperature=0.3)\n",
    "        self.memory_file = memory_file\n",
    "        \n",
    "        # Initialize components\n",
    "        self.kg_builder = KnowledgeGraphBuilder()\n",
    "        self.fetcher = FinancialDataFetcher()\n",
    "        self.chaining_workflow = PromptChainWorkflow(llm)\n",
    "        self.routing_workflow = RoutingWorkflow(llm)\n",
    "        self.eo_workflow = EvaluatorOptimizerWorkflow(llm, max_iterations=2)\n",
    "    \n",
    "    def step1_plan_research(self, ticker: str, user_context: str = \"\") -> ResearchPlan:\n",
    "        \"\"\"AGENT FUNCTION 1: Plans research steps autonomously.\"\"\"\n",
    "        print(\"\\n=== STEP 1: PLANNING RESEARCH ===\")\n",
    "        \n",
    "        # Load past learnings\n",
    "        past_learnings = self._load_memory()\n",
    "        \n",
    "        structured_llm = self.llm.with_structured_output(ResearchPlan)\n",
    "        \n",
    "        prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", \"\"\"You are an autonomous investment research agent. Plan comprehensive research.\n",
    "\n",
    "Your research plan should:\n",
    "1. Break down the research into clear, ordered steps\n",
    "2. Identify required data sources\n",
    "3. List key questions that must be answered\n",
    "4. Estimate research complexity\n",
    "\n",
    "Consider past learnings to improve your approach:\n",
    "{past_learnings}\n",
    "\n",
    "Be thorough, systematic, and strategic.\"\"\"),\n",
    "            (\"user\", \"Ticker: {ticker}\\n\\nUser Context: {user_context}\\n\\nCreate a research plan:\")\n",
    "        ])\n",
    "        \n",
    "        chain = prompt | structured_llm\n",
    "        plan = chain.invoke({\n",
    "            \"ticker\": ticker,\n",
    "            \"user_context\": user_context or \"General investment analysis\",\n",
    "            \"past_learnings\": past_learnings\n",
    "        })\n",
    "        \n",
    "        print(f\"✓ Research plan created with {len(plan.research_steps)} steps\")\n",
    "        print(f\"  Complexity: {plan.estimated_complexity}\")\n",
    "        print(f\"  Data sources: {', '.join(plan.data_sources_needed)}\")\n",
    "        \n",
    "        return plan\n",
    "    \n",
    "    def step2_execute_research(self, plan: ResearchPlan) -> Dict:\n",
    "        \"\"\"AGENT FUNCTION 2: Uses tools dynamically based on plan.\"\"\"\n",
    "        print(\"\\n=== STEP 2: EXECUTING RESEARCH ===\")\n",
    "        results = {}\n",
    "        \n",
    "        # Tool 1: Build knowledge graph\n",
    "        if \"knowledge graph\" in \" \".join(plan.data_sources_needed).lower() or \\\n",
    "           \"entity extraction\" in \" \".join(plan.data_sources_needed).lower():\n",
    "            \n",
    "            print(\"\\n[Tool: Knowledge Graph] Building entity graph...\")\n",
    "            graph = self.kg_builder.expand_from_seed(\n",
    "                plan.ticker,\n",
    "                seed_label=\"STOCK_SYMBOL\",\n",
    "                depth=2\n",
    "            )\n",
    "            results[\"knowledge_graph\"] = self.kg_builder.get_graph_summary()\n",
    "            print(f\"✓ Knowledge graph: {graph.number_of_nodes()} nodes\")\n",
    "        \n",
    "        # Tool 2: Fetch and analyze news\n",
    "        if \"news\" in \" \".join(plan.data_sources_needed).lower():\n",
    "            print(\"\\n[Tool: News Analysis] Fetching recent news...\")\n",
    "            news_data = self.fetcher.fetch_yahoo_news(plan.ticker)\n",
    "            \n",
    "            if news_data and len(news_data) > 100:\n",
    "                print(\"[Workflow: Prompt Chaining] Processing news...\")\n",
    "                news_summary = self.chaining_workflow.run(news_data, source=\"Yahoo Finance\")\n",
    "                results[\"news_analysis\"] = {\n",
    "                    \"summary\": news_summary.executive_summary,\n",
    "                    \"key_points\": news_summary.key_points,\n",
    "                    \"category\": news_summary.classification.category,\n",
    "                    \"sentiment\": news_summary.classification.sentiment\n",
    "                }\n",
    "                print(f\"✓ News analyzed: {news_summary.classification.category} / {news_summary.classification.sentiment}\")\n",
    "        \n",
    "        # Tool 3: Route to specialized analyzer\n",
    "        if \"earnings\" in \" \".join(plan.data_sources_needed).lower() or \\\n",
    "           \"market analysis\" in \" \".join(plan.data_sources_needed).lower():\n",
    "            \n",
    "            print(\"\\n[Workflow: Routing] Routing to specialist...\")\n",
    "            content = self.fetcher.fetch_alpha_vantage_quote(plan.ticker)\n",
    "            \n",
    "            if content and len(content) > 50:\n",
    "                routing_result = self.routing_workflow.process(\n",
    "                    content,\n",
    "                    title=f\"{plan.ticker} Financial Data\"\n",
    "                )\n",
    "                results[\"specialized_analysis\"] = {\n",
    "                    \"route\": routing_result[\"routing_decision\"].route,\n",
    "                    \"analysis_type\": routing_result[\"analysis_type\"]\n",
    "                }\n",
    "                print(f\"✓ Routed to {routing_result['routing_decision'].route} analyst\")\n",
    "        \n",
    "        # Tool 4: Generate comprehensive investment analysis\n",
    "        print(\"\\n[Workflow: Evaluator-Optimizer] Generating investment analysis...\")\n",
    "        \n",
    "        compiled_data = self._compile_data_for_analysis(plan.ticker, results)\n",
    "        \n",
    "        eo_result = self.eo_workflow.run(\n",
    "            plan.ticker,\n",
    "            compiled_data,\n",
    "            verbose=False\n",
    "        )\n",
    "        \n",
    "        results[\"investment_analysis\"] = {\n",
    "            \"recommendation\": eo_result[\"final_analysis\"].recommendation,\n",
    "            \"target_price\": eo_result[\"final_analysis\"].target_price,\n",
    "            \"thesis\": eo_result[\"final_analysis\"].investment_thesis,\n",
    "            \"quality_score\": eo_result[\"final_evaluation\"].overall_score,\n",
    "            \"iterations\": eo_result[\"iterations_performed\"]\n",
    "        }\n",
    "        \n",
    "        print(f\"✓ Investment analysis: {eo_result['final_analysis'].recommendation} \"\n",
    "              f\"(quality: {eo_result['final_evaluation'].overall_score:.1f}/100)\")\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def step3_reflect_on_quality(self, plan: ResearchPlan, results: Dict) -> ResearchReflection:\n",
    "        \"\"\"AGENT FUNCTION 3: Self-reflects on research quality.\"\"\"\n",
    "        print(\"\\n=== STEP 3: SELF-REFLECTION ===\")\n",
    "        \n",
    "        structured_llm = self.llm.with_structured_output(ResearchReflection)\n",
    "        \n",
    "        prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", \"\"\"You are reflecting on the quality of your own research. Be critical and honest.\n",
    "\n",
    "Evaluate:\n",
    "- Completeness: Did we answer all key questions?\n",
    "- Confidence: How confident are we in the findings?\n",
    "- Data Quality: Were the data sources reliable and sufficient?\n",
    "\n",
    "Identify:\n",
    "- Strengths: What did we do well?\n",
    "- Gaps: What's missing or incomplete?\n",
    "- Reliability concerns: Any data quality issues?\n",
    "\n",
    "Provide honest self-assessment and concrete improvement recommendations.\"\"\"),\n",
    "            (\"user\", \"\"\"Research Plan:\n",
    "Ticker: {ticker}\n",
    "Steps: {steps}\n",
    "Key Questions: {questions}\n",
    "\n",
    "Research Results:\n",
    "{results}\n",
    "\n",
    "Reflect on the quality of this research:\"\"\")\n",
    "        ])\n",
    "        \n",
    "        chain = prompt | structured_llm\n",
    "        reflection = chain.invoke({\n",
    "            \"ticker\": plan.ticker,\n",
    "            \"steps\": \"\\n\".join(f\"- {step}\" for step in plan.research_steps),\n",
    "            \"questions\": \"\\n\".join(f\"- {q}\" for q in plan.key_questions),\n",
    "            \"results\": str(results)[:3000]  # Limit length\n",
    "        })\n",
    "        \n",
    "        print(f\"✓ Self-reflection complete\")\n",
    "        print(f\"  Completeness: {reflection.completeness_score:.1f}/100\")\n",
    "        print(f\"  Confidence: {reflection.confidence_score:.1f}/100\")\n",
    "        print(f\"  Data Quality: {reflection.data_quality_score:.1f}/100\")\n",
    "        print(f\"  Gaps identified: {len(reflection.gaps)}\")\n",
    "        \n",
    "        return reflection\n",
    "    \n",
    "    def step4_learn_from_run(self, reflection: ResearchReflection):\n",
    "        \"\"\"AGENT FUNCTION 4: Learns across runs.\"\"\"\n",
    "        print(\"\\n=== STEP 4: LEARNING ===\")\n",
    "        \n",
    "        learning_entry = f\"\"\"\n",
    "Run: {reflection.overall_assessment}\n",
    "Improvements to apply:\n",
    "{chr(10).join(f\"- {rec}\" for rec in reflection.improvement_recommendations)}\n",
    "Gaps to avoid:\n",
    "{chr(10).join(f\"- {gap}\" for gap in reflection.gaps)}\n",
    "---\n",
    "\"\"\"\n",
    "        \n",
    "        self._save_to_memory(learning_entry)\n",
    "        print(f\"✓ Learnings saved to {self.memory_file}\")\n",
    "        print(f\"  {len(reflection.improvement_recommendations)} improvement recommendations stored\")\n",
    "    \n",
    "    def research_stock(self, ticker: str, user_context: str = \"\") -> ResearchResult:\n",
    "        \"\"\"Execute complete autonomous research workflow.\"\"\"\n",
    "        import time\n",
    "        start_time = time.time()\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(f\" AUTONOMOUS RESEARCH AGENT: {ticker}\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        # Step 1: Plan\n",
    "        plan = self.step1_plan_research(ticker, user_context)\n",
    "        \n",
    "        # Step 2: Execute with dynamic tool selection\n",
    "        results = self.step2_execute_research(plan)\n",
    "        \n",
    "        # Step 3: Self-reflect\n",
    "        reflection = self.step3_reflect_on_quality(plan, results)\n",
    "        \n",
    "        # Step 4: Learn\n",
    "        self.step4_learn_from_run(reflection)\n",
    "        \n",
    "        execution_time = time.time() - start_time\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(f\" RESEARCH COMPLETE ({execution_time:.1f}s)\")\n",
    "        print(\"=\"*80 + \"\\n\")\n",
    "        \n",
    "        return ResearchResult(\n",
    "            ticker=ticker,\n",
    "            plan=plan,\n",
    "            knowledge_graph_summary=results.get(\"knowledge_graph\", \"Not generated\"),\n",
    "            news_analysis=results.get(\"news_analysis\"),\n",
    "            specialized_analysis=results.get(\"specialized_analysis\"),\n",
    "            investment_analysis=results.get(\"investment_analysis\"),\n",
    "            reflection=reflection,\n",
    "            execution_time_seconds=execution_time\n",
    "        )\n",
    "    \n",
    "    def _compile_data_for_analysis(self, ticker: str, results: Dict) -> str:\n",
    "        \"\"\"Compile research results into formatted text for analysis.\"\"\"\n",
    "        sections = [f\"{ticker} Research Data\\n\"]\n",
    "        \n",
    "        if \"knowledge_graph\" in results:\n",
    "            sections.append(f\"Knowledge Graph:\\n{results['knowledge_graph']}\\n\")\n",
    "        \n",
    "        if \"news_analysis\" in results:\n",
    "            news = results[\"news_analysis\"]\n",
    "            sections.append(f\"News Analysis:\\n\")\n",
    "            sections.append(f\"Summary: {news.get('summary', 'N/A')}\\n\")\n",
    "            sections.append(f\"Sentiment: {news.get('sentiment', 'N/A')}\\n\")\n",
    "        \n",
    "        # Add fetched financial data\n",
    "        quote_data = self.fetcher.fetch_alpha_vantage_quote(ticker)\n",
    "        if quote_data:\n",
    "            sections.append(f\"\\n{quote_data}\\n\")\n",
    "        \n",
    "        return \"\\n\".join(sections)\n",
    "    \n",
    "    def _load_memory(self) -> str:\n",
    "        \"\"\"Load past learnings from memory file.\"\"\"\n",
    "        try:\n",
    "            with open(self.memory_file, 'r') as f:\n",
    "                content = f.read()\n",
    "                return content[-2000:] if content else \"No past learnings yet.\"\n",
    "        except FileNotFoundError:\n",
    "            return \"No past learnings yet.\"\n",
    "    \n",
    "    def _save_to_memory(self, entry: str):\n",
    "        \"\"\"Append learning entry to memory file.\"\"\"\n",
    "        with open(self.memory_file, 'a') as f:\n",
    "            f.write(entry)\n",
    "\n",
    "\n",
    "print(\"✓ AutonomousResearchAgent class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Interactive Demonstration\n",
    "\n",
    "Run the autonomous research agent on a stock ticker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the autonomous research agent\n",
    "agent = AutonomousResearchAgent()\n",
    "\n",
    "# Research a stock (you can change the ticker and context)\n",
    "ticker = \"AAPL\"  # Change this to any stock ticker\n",
    "context = \"Looking for long-term growth potential in tech sector\"\n",
    "\n",
    "result = agent.research_stock(ticker=ticker, user_context=context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display Research Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\" RESEARCH SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nTicker: {result.ticker}\")\n",
    "print(f\"Execution Time: {result.execution_time_seconds:.1f} seconds\")\n",
    "\n",
    "print(f\"\\n--- Research Plan ---\")\n",
    "for i, step in enumerate(result.plan.research_steps, 1):\n",
    "    print(f\"  {i}. {step}\")\n",
    "\n",
    "print(f\"\\nEstimated Complexity: {result.plan.estimated_complexity}\")\n",
    "print(f\"Data Sources Used: {', '.join(result.plan.data_sources_needed)}\")\n",
    "\n",
    "if result.investment_analysis:\n",
    "    print(f\"\\n--- Investment Analysis ---\")\n",
    "    print(f\"Recommendation: {result.investment_analysis['recommendation']}\")\n",
    "    print(f\"Target Price: ${result.investment_analysis['target_price']}\")\n",
    "    print(f\"Quality Score: {result.investment_analysis['quality_score']:.1f}/100\")\n",
    "    print(f\"Refinement Iterations: {result.investment_analysis['iterations']}\")\n",
    "\n",
    "if result.news_analysis:\n",
    "    print(f\"\\n--- News Analysis ---\")\n",
    "    print(f\"Summary: {result.news_analysis['summary']}\")\n",
    "    print(f\"Sentiment: {result.news_analysis['sentiment']}\")\n",
    "    print(f\"Category: {result.news_analysis['category']}\")\n",
    "\n",
    "print(f\"\\n--- Self-Reflection ---\")\n",
    "print(f\"Completeness: {result.reflection.completeness_score:.1f}/100\")\n",
    "print(f\"Confidence: {result.reflection.confidence_score:.1f}/100\")\n",
    "print(f\"Data Quality: {result.reflection.data_quality_score:.1f}/100\")\n",
    "\n",
    "if result.reflection.gaps:\n",
    "    print(f\"\\nIdentified Gaps:\")\n",
    "    for gap in result.reflection.gaps[:3]:\n",
    "        print(f\"  • {gap}\")\n",
    "\n",
    "if result.reflection.improvement_recommendations:\n",
    "    print(f\"\\nImprovement Recommendations:\")\n",
    "    for rec in result.reflection.improvement_recommendations[:3]:\n",
    "        print(f\"  • {rec}\")\n",
    "\n",
    "print(f\"\\nOverall Assessment: {result.reflection.overall_assessment}\")\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook demonstrates:\n",
    "\n",
    "1. **Three Agentic Workflow Patterns**:\n",
    "   - Prompt Chaining for sequential processing\n",
    "   - Routing for intelligent task distribution\n",
    "   - Evaluator-Optimizer for iterative refinement\n",
    "\n",
    "2. **Autonomous Agent Capabilities**:\n",
    "   - Planning research steps\n",
    "   - Dynamic tool selection and execution\n",
    "   - Self-reflection on output quality\n",
    "   - Learning across runs\n",
    "\n",
    "3. **Integration of Multiple Components**:\n",
    "   - Knowledge graph construction\n",
    "   - Financial data fetching from multiple sources\n",
    "   - LLM-powered analysis and reasoning\n",
    "   - Structured output with Pydantic models\n",
    "\n",
    "You can modify the code cells above to:\n",
    "- Research different stock tickers\n",
    "- Adjust workflow parameters\n",
    "- Experiment with different prompts\n",
    "- Add new data sources or analysis methods"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
