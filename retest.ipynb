{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b39d4010",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import sqlite3\n",
    "from typing import List, Dict, Any, Set, Tuple\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90d57526",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9e76aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import SystemMessage, HumanMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4dab8a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ddgs import DDGS\n",
    "import wikipedia\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9e61851",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    from langchain_community.tools.yahoo_finance_news import YahooFinanceNewsTool\n",
    "    _HAS_YAHOO_TOOL = True\n",
    "except Exception:\n",
    "    _HAS_YAHOO_TOOL = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b31a0fa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d9/34q0n1js31x765w00t009x6c0000gn/T/ipykernel_21636/95821855.py:2: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
      "  llm = ChatOpenAI(temperature=0.0, model_name=MODEL_NAME, base_url=\"http://localhost:1234/v1\", api_key=\"lm-studio\")\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = \"gemma-3-27b-it-qat\"\n",
    "llm = ChatOpenAI(temperature=0.0, model_name=MODEL_NAME, base_url=\"http://localhost:1234/v1\", api_key=\"lm-studio\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d1af9aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "DB_PATH = \"agentic_research_graph.db\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a75a1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "NER_PROMPT = (\n",
    "    \"You are a precise Named Entity extractor specialized for financial research.\\n\"\n",
    "    \"Given this text, return ONLY a JSON array of entities. Each entity object must contain:\\n\"\n",
    "    \" - name (string)\\n\"\n",
    "    \" - type (one of: company, product, person, policy, regulation, industry, market, indicator, other)\\n\"\n",
    "    \" - relevance (integer 1-10)\\n\"\n",
    "    \" - short_description (<=30 words)\\n\"\n",
    "    \" - tags (array of short strings)\\n\"\n",
    "    \"Do NOT return any prose or commentary — only the JSON array.\\n\\nText:\\n<<TEXT>>\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8e4801e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_entities_with_llm(text: str, cap_chars: int = 3000) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Run the LLM to extract entities from text. Caps text length for safety.\"\"\"\n",
    "    if not text:\n",
    "        return []\n",
    "    text_in = text if len(text) <= cap_chars else text[:cap_chars]\n",
    "    prompt = NER_PROMPT.replace(\"<<TEXT>>\", text_in)\n",
    "    resp = llm([SystemMessage(content=\"You are a strict JSON-only extractor.\"), HumanMessage(content=prompt)])\n",
    "    out = resp.content\n",
    "    # Try to parse JSON strictly, else try to extract first JSON array.\n",
    "    try:\n",
    "        parsed = json.loads(out)\n",
    "        if isinstance(parsed, list):\n",
    "            return parsed\n",
    "    except Exception:\n",
    "        import re\n",
    "        m = re.search(r\"(\\[.*\\])\", out, flags=re.S)\n",
    "        if m:\n",
    "            try:\n",
    "                return json.loads(m.group(1))\n",
    "            except Exception:\n",
    "                pass\n",
    "    return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a33a2eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ddg_search(query: str, max_results: int = 5):\n",
    "    try:\n",
    "        with DDGS() as ddg:\n",
    "            results = [r for r in ddg.text(query, max_results=max_results)]\n",
    "        return results\n",
    "    except Exception as e:\n",
    "        return [{\"error\": str(e)}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "343cbca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wikipedia_summary(query: str, sentences: int = 3):\n",
    "    import wikipedia\n",
    "    try:\n",
    "        # Try exact page, then fallback to search\n",
    "        try:\n",
    "            return wikipedia.summary(query, sentences=sentences, auto_suggest=False)\n",
    "        except wikipedia.exceptions.DisambiguationError as e:\n",
    "            alt = e.options[0]\n",
    "            return wikipedia.summary(alt, sentences=sentences)\n",
    "        except wikipedia.exceptions.PageError:\n",
    "            search = wikipedia.search(query)\n",
    "            if search:\n",
    "                return wikipedia.summary(search[0], sentences=sentences)\n",
    "            raise\n",
    "    except Exception as e:\n",
    "        return f\"WIKIPEDIA_ERROR: {e}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0c38a0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def yahoo_finance_news_tool(ticker_or_query: str, limit: int = 8):\n",
    "    from langchain_community.tools.yahoo_finance_news import YahooFinanceNewsTool\n",
    "    import yfinance as yf\n",
    "    try:\n",
    "        tool = YahooFinanceNewsTool()\n",
    "        if len(ticker_or_query) <= 5:  # ticker\n",
    "            tk = yf.Ticker(ticker_or_query)\n",
    "            company_name = tk.info.get(\"longName\", ticker_or_query)\n",
    "            out = tool.run(company_name)\n",
    "        else:\n",
    "            out = tool.run(ticker_or_query)\n",
    "        if isinstance(out, list):\n",
    "            return out[:limit]\n",
    "        if isinstance(out, dict):\n",
    "            return out.get(\"items\", [out])[:limit]\n",
    "        return [{\"text\": str(out)}]\n",
    "    except Exception as e:\n",
    "        return [{\"error\": str(e)}]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "01989c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_db(db_path: str = DB_PATH) -> sqlite3.Connection:\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(\"\"\"CREATE TABLE IF NOT EXISTS nodes (\n",
    "                    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "                    name TEXT UNIQUE,\n",
    "                    depth INTEGER,\n",
    "                    source TEXT,\n",
    "                    notes TEXT\n",
    "                )\"\"\")\n",
    "    cur.execute(\"\"\"CREATE TABLE IF NOT EXISTS edges (\n",
    "                    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "                    parent INTEGER,\n",
    "                    child INTEGER,\n",
    "                    relation TEXT\n",
    "                )\"\"\")\n",
    "    conn.commit()\n",
    "    return conn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4f08b619",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_node(conn: sqlite3.Connection, name: str, depth: int, source: str, notes: Dict[str, Any]) -> int:\n",
    "    cur = conn.cursor()\n",
    "    notes_json = json.dumps(notes, default=str)\n",
    "    cur.execute(\"INSERT OR IGNORE INTO nodes (name, depth, source, notes) VALUES (?, ?, ?, ?)\",\n",
    "                (name, depth, source, notes_json))\n",
    "    conn.commit()\n",
    "    cur.execute(\"SELECT id FROM nodes WHERE name = ?\", (name,))\n",
    "    row = cur.fetchone()\n",
    "    return int(row[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8f3a4d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_edge(conn: sqlite3.Connection, parent_id: int, child_id: int, relation: str = \"related\"):\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(\"INSERT INTO edges (parent, child, relation) VALUES (?, ?, ?)\", (parent_id, child_id, relation))\n",
    "    conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "71c75762",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_graph_from_db(conn: sqlite3.Connection) -> nx.DiGraph:\n",
    "    G = nx.DiGraph()\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(\"SELECT id, name, depth FROM nodes\")\n",
    "    for nid, name, depth in cur.fetchall():\n",
    "        G.add_node(nid, name=name, depth=depth)\n",
    "    cur.execute(\"SELECT parent, child, relation FROM edges\")\n",
    "    for parent, child, rel in cur.fetchall():\n",
    "        G.add_edge(parent, child, relation=rel)\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5689c040",
   "metadata": {},
   "outputs": [],
   "source": [
    "def planner_agent(question: str, ticker: str) -> List[str]:\n",
    "    prompt = (\n",
    "        f\"You are an investment research planner. Given the user's question: \\\"{question}\\\" \"\n",
    "        f\"and ticker: {ticker}, produce a concise numbered plan of 6 steps to research this question. \"\n",
    "        \"Return the steps as a plain newline-separated list (1., 2., ...).\"\n",
    "    )\n",
    "    resp = llm([SystemMessage(content=\"You are a concise planner.\"), HumanMessage(content=prompt)])\n",
    "    text = resp.content.strip()\n",
    "    print(text)\n",
    "    # simple parse into steps\n",
    "    steps = [line.strip() for line in text.splitlines() if line.strip()]\n",
    "    return steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e2c631d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def router_agent(content_snippet: str) -> str:\n",
    "    \"\"\"\n",
    "    Determine which specialist should handle the given content snippet.\n",
    "    Returns one of: 'news', 'earnings', 'market', 'policy', 'wiki', 'general'\n",
    "    \"\"\"\n",
    "    prompt = (\n",
    "        \"You are a router that decides which specialist should handle a snippet.\\n\"\n",
    "        \"Given this short text, return only one word tag from: news, earnings, market, policy, wiki, general.\\n\\n\"\n",
    "        f\"Text:\\n{content_snippet[:1000]}\\n\"\n",
    "    )\n",
    "    resp = llm([SystemMessage(content=\"You are a single-word router.\"), HumanMessage(content=prompt)])\n",
    "    tag = resp.content.strip().lower().split()[0]\n",
    "    if tag not in {\"news\", \"earnings\", \"market\", \"policy\", \"wiki\", \"general\"}:\n",
    "        return \"general\"\n",
    "    return tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0dcf5a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def news_specialist_agent(news_item: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"Analyze a single news item: produce summary, sentiment, named entities (via extractor)\"\"\"\n",
    "    title = news_item.get(\"title\") or news_item.get(\"headline\") or news_item.get(\"text\") or str(news_item)\n",
    "    body = news_item.get(\"summary\") or news_item.get(\"snippet\") or news_item.get(\"text\") or \"\"\n",
    "    combined = f\"TITLE: {title}\\n\\n{body}\"\n",
    "    # Summarize + sentiment + NER\n",
    "    summary_prompt = (\n",
    "        \"You are a financial news analyst. Provide:\\n\"\n",
    "        \"1) one-line summary (<=25 words)\\n\"\n",
    "        \"2) sentiment: positive/neutral/negative\\n\"\n",
    "        \"Return JSON: {\\\"summary\\\":..., \\\"sentiment\\\":...}\\n\\n\"\n",
    "        f\"News:\\n{combined}\"\n",
    "    )\n",
    "    resp = llm([SystemMessage(content=\"You are a JSON-only news analyzer.\"), HumanMessage(content=summary_prompt)])\n",
    "    out = resp.content\n",
    "    try:\n",
    "        res = json.loads(out)\n",
    "    except Exception:\n",
    "        # attempt to parse lines\n",
    "        res = {\"summary\": combined[:200], \"sentiment\": \"neutral\", \"raw\": out}\n",
    "    # NER from combined text\n",
    "    entities = extract_entities_with_llm(combined)\n",
    "    res[\"entities\"] = entities\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "df460c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def earnings_specialist_agent(ticker: str) -> Dict[str, Any]:\n",
    "    \"\"\"Analyze earnings/financials using yfinance\"\"\"\n",
    "    try:\n",
    "        tk = yf.Ticker(ticker)\n",
    "        info = getattr(tk, \"info\", {}) or {}\n",
    "        financials = getattr(tk, \"financials\", {}) or {}\n",
    "        # ask LLM to summarize financials and flag signals\n",
    "        fin_text = f\"Company info: {json.dumps(info, default=str)[:2000]}\\n\\nFinancials (truncated): {str(financials)[:2000]}\"\n",
    "        prompt = (\n",
    "            \"You are an earnings specialist. Read the company info and financials and return a short JSON:\\n\"\n",
    "            \"{\\\"summary\\\":\\\"...\\\",\\\"key_signals\\\":[\\\"...\\\"], \\\"risk_flags\\\":[\\\"...\\\"]}\\n\\n\"\n",
    "            f\"{fin_text}\"\n",
    "        )\n",
    "        resp = llm([SystemMessage(content=\"You are a financial summarizer.\"), HumanMessage(content=prompt)])\n",
    "        try:\n",
    "            return json.loads(resp.content)\n",
    "        except Exception:\n",
    "            return {\"summary\": resp.content}\n",
    "    except Exception as e:\n",
    "        return {\"error\": str(e)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4b4b72db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def market_specialist_agent(ticker: str, context_snippet: str = \"\") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Higher-level market analysis: macro, sector strength, comparables\n",
    "    Uses ddg_search for sector news and LLM for summarization.\n",
    "    \"\"\"\n",
    "    # quick sector search\n",
    "    sector_search = ddg_search(f\"{ticker} sector performance outlook\", max_results=4)\n",
    "    prompt = (\n",
    "        \"You are a market analyst. Given the snippets below, produce JSON:\\n\"\n",
    "        \"{\\\"summary\\\":\\\"...\\\",\\\"macro_drivers\\\":[\\\"...\\\"],\\\"sector_trend\\\":\\\"up/flat/down\\\"}\\n\\n\"\n",
    "        f\"Snippets: {sector_search}\\n\\nContext: {context_snippet[:800]}\"\n",
    "    )\n",
    "    resp = llm([SystemMessage(content=\"You are a market analyst.\"), HumanMessage(content=prompt)])\n",
    "    try:\n",
    "        return json.loads(resp.content)\n",
    "    except Exception:\n",
    "        return {\"summary\": resp.content}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "203d6aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_specialist_agent(entity: str) -> Dict[str, Any]:\n",
    "    \"\"\"Search for government policy / subsidy terms related to the entity and summarize implication\"\"\"\n",
    "    gov_search = ddg_search(f\"{entity} government policy subsidy regulation\", max_results=4)\n",
    "    prompt = (\n",
    "        \"You are a policy analyst. Given search results, summarize whether governments are promoting or restricting growth, \"\n",
    "        \"and list any named policies found. Return JSON: {\\\"promotion\\\":true/false, \\\"policies\\\":[...], \\\"notes\\\":\\\"...\\\"}\\n\\n\"\n",
    "        f\"Results: {gov_search}\"\n",
    "    )\n",
    "    resp = llm([SystemMessage(content=\"You are a policy analyst.\"), HumanMessage(content=prompt)])\n",
    "    try:\n",
    "        return json.loads(resp.content)\n",
    "    except Exception:\n",
    "        return {\"summary\": resp.content, \"raw_results\": gov_search}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5e904128",
   "metadata": {},
   "outputs": [],
   "source": [
    "VISITED_ENTITIES: Set[str] = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7a2b8441",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, name: str, source: str, depth: int):\n",
    "        self.name = name\n",
    "        self.source = source\n",
    "        self.depth = depth\n",
    "        self.children: list[\"Node\"] = []\n",
    "        self.notes: dict = {}\n",
    "\n",
    "    def to_dict(self):\n",
    "        return {\n",
    "            \"name\": self.name,\n",
    "            \"source\": self.source,\n",
    "            \"depth\": self.depth,\n",
    "            \"notes\": self.notes,\n",
    "            \"children\": [c.to_dict() for c in self.children],\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b15fe12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def investigate_entity(entity_name: str, depth: int, max_depth: int, conn: sqlite3.Connection) -> Node:\n",
    "    node = Node(entity_name, source='entity', depth=depth)\n",
    "    name_key = entity_name.strip().lower()\n",
    "    print(f\"Visiting entity {entity_name} at depth {depth}\")\n",
    "    if depth >= max_depth or name_key in VISITED_ENTITIES:\n",
    "        return node\n",
    "    VISITED_ENTITIES.add(name_key)\n",
    "\n",
    "    # For depth == 0: we already ran Yahoo News at root\n",
    "    # For depth >= 1: skip Yahoo News and rely on open web + Wikipedia\n",
    "    if depth == 0:\n",
    "        # Already handled in root level, but you can include fallback here if needed\n",
    "        pass\n",
    "    else:\n",
    "        # Internet-based exploration (for non-ticker entities)\n",
    "        ddg_res_raw = ddg_search(entity_name, max_results=5)\n",
    "        try:\n",
    "            ddg_res = json.loads(ddg_res_raw)\n",
    "        except Exception:\n",
    "            ddg_res = [{\"error\": ddg_res_raw}]\n",
    "        node.notes['ddg'] = ddg_res\n",
    "\n",
    "        # Wikipedia summary\n",
    "        wiki = wikipedia_summary(entity_name)\n",
    "        node.notes['wikipedia'] = wiki\n",
    "\n",
    "        # Government/policy relevance\n",
    "        gov_search_raw = ddg_search(f\"{entity_name} government policy\", max_results=3)\n",
    "        try:\n",
    "            gov_search = json.loads(gov_search_raw)\n",
    "        except Exception:\n",
    "            gov_search = [{\"error\": gov_search_raw}]\n",
    "        node.notes['gov_search'] = gov_search\n",
    "\n",
    "    # Aggregate for NER extraction\n",
    "    snippets = []\n",
    "    if node.notes.get('ddg'):\n",
    "        for r in node.notes['ddg'][:5]:\n",
    "            if isinstance(r, dict):\n",
    "                snippets.append(r.get('body') or r.get('snippet') or r.get('title') or '')\n",
    "            else:\n",
    "                snippets.append(str(r))\n",
    "    agg_text = '\\n'.join(snippets) + '\\n' + node.notes.get('wikipedia', '')\n",
    "    entities = extract_entities_with_llm(agg_text)\n",
    "    node.notes['extracted_entities'] = entities\n",
    "\n",
    "    # Store node\n",
    "    node_id = add_node(conn, entity_name, depth, 'investigate_entity', node.notes)\n",
    "\n",
    "    # Recursive exploration for newly found entities\n",
    "    for ent in entities:\n",
    "        child_name = ent.get('name')\n",
    "        if child_name and isinstance(child_name, str):\n",
    "            child_node = investigate_entity(child_name, depth + 1, max_depth, conn)\n",
    "            child_id = add_node(conn, child_node.name, child_node.depth, child_node.source, child_node.notes)\n",
    "            add_edge(conn, node_id, child_id)\n",
    "            node.children.append(child_node)\n",
    "\n",
    "    return node\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9468847f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluator_agent(tree_json: Dict[str, Any], question: str, ticker: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Produce a structured recommendation (decision: BUY/HOLD/SELL, confidence 1-10, reasons [3]).\n",
    "    Returns a JSON-like dict.\n",
    "    \"\"\"\n",
    "    # Limit tree size in prompt to avoid token explosion\n",
    "    tree_str = json.dumps(tree_json)[:11000]\n",
    "    prompt = (\n",
    "        \"You are an experienced investment analyst. Given the research tree (JSON truncated) and the user's question, \"\n",
    "        \"produce a JSON with fields: decision (BUY/HOLD/SELL), confidence (1-10 integer), reasons (array of 3 short sentences).\\n\\n\"\n",
    "        f\"Question: {question}\\nTicker: {ticker}\\nResearchTree: {tree_str}\\n\"\n",
    "    )\n",
    "    resp = llm([SystemMessage(content=\"You are an objective analyst.\"), HumanMessage(content=prompt)])\n",
    "    out = resp.content\n",
    "    try:\n",
    "        return json.loads(out)\n",
    "    except Exception:\n",
    "        import re\n",
    "        m = re.search(r\"(\\{.*\\})\", out, flags=re.S)\n",
    "        if m:\n",
    "            try:\n",
    "                return json.loads(m.group(1))\n",
    "            except Exception:\n",
    "                pass\n",
    "        return {\"raw\": out}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "088b63d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimizer_agent(prev_recommendation: Dict[str, Any], tree_json: Dict[str, Any],\n",
    "                    question: str, ticker: str) -> Tuple[Dict[str, Any], bool]:\n",
    "    \"\"\"\n",
    "    If evaluator confidence is low (<6) or reasons seem weak, ask the LLM to propose 1-2 extra research tasks\n",
    "    (e.g., deeper ddg searches, check SEC filings, check specific policies) and return them.\n",
    "    Returns (instructions, should_run) where instructions is a dict describing new work and should_run boolean.\n",
    "    \"\"\"\n",
    "    confidence = prev_recommendation.get(\"confidence\")\n",
    "    if isinstance(confidence, int) and confidence >= 6:\n",
    "        return ({\"note\": \"Confidence adequate; no extra steps.\"}, False)\n",
    "\n",
    "    prompt = (\n",
    "        \"You are an optimizer. Given a previous recommendation and the research tree, propose 2 concrete extra research steps \"\n",
    "        \"that could improve confidence (e.g., search specific filings, deep-dive keywords, check recent earnings call transcript).\\n\"\n",
    "        \"Return JSON: {\\\"extra_steps\\\": [\\\"step1\\\", \\\"step2\\\"]}\\n\\n\"\n",
    "        f\"PrevRecommendation: {json.dumps(prev_recommendation)}\\nTree: {json.dumps(tree_json)[:8000]}\"\n",
    "    )\n",
    "    resp = llm([SystemMessage(content=\"You are a research optimizer.\"), HumanMessage(content=prompt)])\n",
    "    out = resp.content\n",
    "    try:\n",
    "        parsed = json.loads(out)\n",
    "        steps = parsed.get(\"extra_steps\", [])\n",
    "        return ({\"extra_steps\": steps}, True if steps else False)\n",
    "    except Exception:\n",
    "        return ({\"raw\": out}, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6e324e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_agentic_pipeline(question: str, ticker: str, max_depth: int = 3, optimizer_rounds: int = 1) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Agentic pipeline:\n",
    "      1) Planner generates a plan (for display only)\n",
    "      2) Investigate recursively using Router + Specialists + Extractor (persists in SQLite)\n",
    "      3) Evaluator produces recommendation\n",
    "      4) Optimizer may propose extra steps; if so, run them and re-evaluate\n",
    "    \"\"\"\n",
    "    conn = init_db(DB_PATH)\n",
    "    graph = nx.DiGraph()\n",
    "    # Reset visited set (so multiple runs in same session behave)\n",
    "    global VISITED_ENTITIES\n",
    "    VISITED_ENTITIES = set()\n",
    "\n",
    "    # Planner\n",
    "    plan_steps = planner_agent(question, ticker)\n",
    "\n",
    "    # Root investigation\n",
    "    root_tree = investigate_entity(ticker, depth=0, max_depth=max_depth, conn=conn)\n",
    "\n",
    "    # Evaluate\n",
    "    recommendation = evaluator_agent(root_tree, question, ticker)\n",
    "\n",
    "    # Optimizer loop\n",
    "    for r in range(optimizer_rounds):\n",
    "        opt_instructions, should_run = optimizer_agent(recommendation, root_tree, question, ticker)\n",
    "        if not should_run:\n",
    "            break\n",
    "        # For each extra step, do a best-effort action: parse the instruction string and run a ddg or wiki search\n",
    "        extra_steps = opt_instructions.get(\"extra_steps\", [])\n",
    "        for step in extra_steps:\n",
    "            # Simple heuristic: if mentions 'SEC' or 'filing', fetch SEC filings (we'll call ddg)\n",
    "            if \"sec\" in step.lower() or \"filing\" in step.lower():\n",
    "                sec_res = ddg_search(f\"{ticker} SEC filings\", max_results=5)\n",
    "                # attach to DB root node notes\n",
    "                root_tree.setdefault(\"notes\", {})\n",
    "                root_tree[\"notes\"].setdefault(\"optimizer_extras\", []).append({\"step\": step, \"results\": sec_res})\n",
    "            else:\n",
    "                # run ddg for the step\n",
    "                ddg_r = ddg_search(step, max_results=5)\n",
    "                root_tree.setdefault(\"notes\", {})\n",
    "                root_tree[\"notes\"].setdefault(\"optimizer_extras\", []).append({\"step\": step, \"results\": ddg_r})\n",
    "        # After running extra steps, re-evaluate\n",
    "        recommendation = evaluator_agent(root_tree, question, ticker)\n",
    "\n",
    "    # Save final networkx graph to a file or return as object\n",
    "    with open(\"investigation_graph.gpickle\", \"wb\") as f:\n",
    "        pickle.dump(graph, f)\n",
    "\n",
    "    return {\n",
    "        \"plan\": plan_steps,\n",
    "        \"tree\": root_tree,\n",
    "        \"recommendation\": recommendation,\n",
    "        \"db_path\": DB_PATH,\n",
    "        \"nx_graph_path\": \"investigation_graph.gpickle\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f3df3a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Should I buy Apple stock today?\"\n",
    "ticker = \"AAPL\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ca4754",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Check AAPL’s current price & recent performance (1wk, 1mo, YTD).\n",
      "2. Review latest news regarding Apple – product launches, earnings reports, major events.\n",
      "3. Analyze key financial metrics: P/E ratio, EPS growth, revenue trends.\n",
      "4. Examine analyst ratings & price targets from reputable sources (e.g., Bloomberg, Reuters).\n",
      "5. Assess overall market conditions & sector outlook (tech industry).\n",
      "6. Consider your personal risk tolerance and investment timeline.\n",
      "Visiting entity AAPL at depth 0\n",
      "Visiting entity BlackRock at depth 1\n",
      "Visiting entity BlackRock, Inc. at depth 2\n",
      "Visiting entity BlackRock, Inc. at depth 3\n",
      "Visiting entity $13.5 trillion at depth 3\n",
      "Visiting entity fixed income at depth 3\n",
      "Visiting entity $13.5 trillion at depth 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/AAI-520-Project/lib/python3.10/site-packages/wikipedia/wikipedia.py:389: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 389 of the file /opt/anaconda3/envs/AAI-520-Project/lib/python3.10/site-packages/wikipedia/wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  lis = BeautifulSoup(html).find_all('li')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visiting entity BlackRock at depth 3\n",
      "Visiting entity CBS Building at depth 3\n",
      "Visiting entity Black Rock City, LLC at depth 3\n",
      "Visiting entity Black Rock Coalition at depth 3\n",
      "Visiting entity Black Rock Studio at depth 3\n",
      "Visiting entity Blackrock College at depth 3\n",
      "Visiting entity Blackrocks Brewery at depth 3\n",
      "Visiting entity Black Rock FC at depth 3\n",
      "Visiting entity Black Rock Football Club at depth 3\n",
      "Visiting entity Black Rock Yacht Club at depth 3\n",
      "Visiting entity Blackrock College RFC at depth 3\n",
      "Visiting entity Blackrock GAA at depth 3\n",
      "Visiting entity Black Rock Rugby Festival at depth 3\n",
      "Visiting entity Blakroc at depth 3\n",
      "Visiting entity Yugen Blakrok at depth 3\n",
      "Visiting entity fixed income at depth 2\n",
      "Visiting entity Fixed Income at depth 3\n",
      "Visiting entity Bonds at depth 3\n",
      "Visiting entity Equity Securities at depth 3\n",
      "Visiting entity Stocks and Shares at depth 3\n",
      "Visiting entity Dividends at depth 3\n",
      "Visiting entity iShares at depth 1\n",
      "Visiting entity iShares at depth 2\n",
      "Visiting entity BlackRock at depth 2\n",
      "Visiting entity Barclays at depth 2\n",
      "Visiting entity Barclays PLC at depth 3\n",
      "Visiting entity London at depth 3\n",
      "Visiting entity UK Consumer Bank at depth 3\n",
      "Visiting entity UK Corporate Bank at depth 3\n",
      "Visiting entity Private Bank and Wealth Management (PBWM) at depth 3\n",
      "Visiting entity Investment Bank at depth 3\n",
      "Visiting entity US Consumer Bank at depth 3\n",
      "Visiting entity City of London at depth 3\n",
      "Visiting entity 1690 at depth 3\n",
      "Visiting entity exchange-traded funds (ETFs) at depth 2\n",
      "Visiting entity exchange-traded fund at depth 3\n",
      "Visiting entity ETF at depth 3\n",
      "Visiting entity stock exchanges at depth 3\n",
      "Visiting entity stocks at depth 3\n",
      "Visiting entity bonds at depth 3\n",
      "Visiting entity currencies at depth 3\n",
      "Visiting entity futures contracts at depth 3\n",
      "Visiting entity commodities at depth 3\n",
      "Visiting entity gold bars at depth 3\n",
      "Visiting entity index mutual funds at depth 2\n",
      "Visiting entity World Equity Benchmark Shares (WEBS) at depth 2\n",
      "Visiting entity bond market at depth 2\n",
      "Visiting entity bond market at depth 3\n",
      "Visiting entity debt market at depth 3\n",
      "Visiting entity credit market at depth 3\n",
      "Visiting entity primary market at depth 3\n",
      "Visiting entity secondary market at depth 3\n",
      "Visiting entity bonds at depth 3\n",
      "Visiting entity notes at depth 3\n",
      "Visiting entity bills at depth 3\n",
      "Visiting entity United States at depth 3\n",
      "Visiting entity stock market at depth 2\n",
      "Visiting entity stock market at depth 3\n",
      "Visiting entity equity market at depth 3\n",
      "Visiting entity share market at depth 3\n",
      "Visiting entity stocks at depth 3\n",
      "Visiting entity public stock exchange at depth 3\n",
      "Visiting entity equity crowdfunding platforms at depth 3\n",
      "Visiting entity US$111 trillion at depth 3\n",
      "Visiting entity US$2.5 trillion at depth 3\n",
      "Visiting entity investment strategy at depth 3\n",
      "Visiting entity S&P 500 at depth 1\n",
      "Visiting entity Standard and Poor's 500 at depth 2\n",
      "Visiting entity S&P 500 at depth 2\n",
      "Visiting entity $57.401 trillion at depth 2\n",
      "Visiting entity United States at depth 2\n",
      "Visiting entity United States of America at depth 3\n",
      "Visiting entity USA at depth 3\n",
      "Visiting entity U.S. at depth 3\n",
      "Visiting entity America at depth 3\n",
      "Visiting entity North America at depth 3\n",
      "Visiting entity Canada at depth 3\n",
      "Visiting entity Mexico at depth 3\n",
      "Visiting entity Alaska at depth 3\n",
      "Visiting entity Hawaii at depth 3\n",
      "Visiting entity Washington, D.C. at depth 3\n",
      "Visiting entity public float weighted/capitalization-weighted index at depth 2\n",
      "Visiting entity capitalization-weighted index at depth 3\n",
      "Visiting entity cap-weighted index at depth 3\n",
      "Visiting entity market-value-weighted index at depth 3\n",
      "Visiting entity stock market index at depth 3\n",
      "Visiting entity outstanding shares at depth 3\n",
      "Visiting entity share price at depth 3\n",
      "Visiting entity market value at depth 3\n",
      "Visiting entity Nasdaq 100 at depth 1\n",
      "Visiting entity Nasdaq-100 at depth 2\n"
     ]
    }
   ],
   "source": [
    "output = run_agentic_pipeline(question, ticker, max_depth=3, optimizer_rounds=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454c5644",
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in output[\"plan\"]:\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8c8eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(json.dumps(output[\"recommendation\"], indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f374b0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"research_tree.json\", \"w\") as f:\n",
    "    json.dump(output[\"tree\"], f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0598a822",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AAI-520-Project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
