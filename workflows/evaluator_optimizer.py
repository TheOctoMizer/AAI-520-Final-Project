"""
Evaluator-Optimizer Workflow Pattern
Generate analysis → Evaluate quality → Refine using feedback.

This pattern demonstrates self-improvement through iterative refinement
based on quality evaluation and constructive criticism.
"""
from typing import Dict, List, Optional
from pydantic import BaseModel, Field
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from utils.llm_factory import create_chat_llm


class InvestmentAnalysis(BaseModel):
    """Investment analysis generated by the analyzer."""
    ticker: str = Field(description="Stock ticker symbol")
    recommendation: str = Field(description="Buy/Hold/Sell recommendation")
    target_price: float = Field(description="Price target")
    investment_thesis: str = Field(description="Core investment thesis")
    key_catalysts: List[str] = Field(description="Key catalysts for the stock")
    risks: List[str] = Field(description="Key risks")
    financial_highlights: str = Field(description="Key financial metrics and trends")
    conclusion: str = Field(description="Summary conclusion")


class QualityEvaluation(BaseModel):
    """Evaluation of analysis quality."""
    overall_score: float = Field(description="Overall quality score 0-100")
    completeness_score: float = Field(description="Completeness 0-100")
    accuracy_score: float = Field(description="Accuracy/logic 0-100")
    actionability_score: float = Field(description="Actionability 0-100")

    strengths: List[str] = Field(description="What the analysis does well")
    weaknesses: List[str] = Field(description="What needs improvement")
    missing_elements: List[str] = Field(description="Important missing information")

    specific_feedback: str = Field(description="Detailed constructive feedback")
    improvement_suggestions: List[str] = Field(description="Specific suggestions to improve")

    is_acceptable: bool = Field(description="Whether analysis meets quality threshold")


class RefinementPlan(BaseModel):
    """Plan for refining the analysis."""
    priority_improvements: List[str] = Field(description="Top priority improvements to make")
    additional_research_needed: List[str] = Field(description="Additional data/research needed")
    areas_to_expand: List[str] = Field(description="Sections that need expansion")
    areas_to_clarify: List[str] = Field(description="Points needing clarification")


class InvestmentAnalyzer:
    """
    Generates initial investment analysis.

    This is the "generator" in the evaluator-optimizer pattern.
    """

    def __init__(self, llm: Optional[ChatOpenAI] = None):
        """Initialize the analyzer."""
        self.llm = llm or create_chat_llm(temperature=0.4)

    def generate(self, ticker: str, company_data: str) -> InvestmentAnalysis:
        """
        Generate initial investment analysis.

        Args:
            ticker: Stock ticker symbol
            company_data: Company and financial data

        Returns:
            InvestmentAnalysis with recommendation
        """
        structured_llm = self.llm.with_structured_output(InvestmentAnalysis)

        prompt = ChatPromptTemplate.from_messages([
            ("system", """You are an investment analyst. Generate a comprehensive stock analysis.

Include:
- Clear Buy/Hold/Sell recommendation
- Price target with justification
- Investment thesis (why this stock?)
- Key catalysts (what will drive the stock?)
- Risks (what could go wrong?)
- Financial highlights (metrics, trends)
- Conclusion

Base your analysis on the provided data. Be specific and actionable."""),
            ("user", "Ticker: {ticker}\n\nCompany Data:\n{company_data}\n\nProvide your analysis:")
        ])

        chain = prompt | structured_llm
        return chain.invoke({"ticker": ticker, "company_data": company_data})


class AnalysisEvaluator:
    """
    Evaluates quality of investment analysis.

    This is the "evaluator" in the evaluator-optimizer pattern.
    It critiques the generated analysis objectively.
    """

    def __init__(self, llm: Optional[ChatOpenAI] = None):
        """Initialize the evaluator."""
        self.llm = llm or create_chat_llm(temperature=0.2)

    def evaluate(self, analysis: InvestmentAnalysis, original_data: str) -> QualityEvaluation:
        """
        Evaluate the quality of an investment analysis.

        Args:
            analysis: The analysis to evaluate
            original_data: Original data used for analysis

        Returns:
            QualityEvaluation with scores and feedback
        """
        structured_llm = self.llm.with_structured_output(QualityEvaluation)

        prompt = ChatPromptTemplate.from_messages([
            ("system", """You are a senior investment analyst reviewing a junior analyst's work.
Evaluate the analysis critically but constructively.

Score each dimension 0-100:
- Completeness: Are all key aspects covered?
- Accuracy: Is the logic sound? Are claims supported?
- Actionability: Can an investor act on this?

Identify:
- Strengths (what's done well)
- Weaknesses (what's lacking)
- Missing elements (what's not covered)
- Specific improvements needed

An analysis is acceptable (is_acceptable=true) only if overall_score >= 75.
Be thorough and specific in your feedback."""),
            ("user", """Original Data:
{original_data}

Analysis to Evaluate:
Ticker: {ticker}
Recommendation: {recommendation}
Target Price: ${target_price}

Investment Thesis:
{investment_thesis}

Key Catalysts:
{catalysts}

Risks:
{risks}

Financial Highlights:
{financials}

Conclusion:
{conclusion}

Provide your evaluation:""")
        ])

        chain = prompt | structured_llm
        return chain.invoke({
            "original_data": original_data,
            "ticker": analysis.ticker,
            "recommendation": analysis.recommendation,
            "target_price": analysis.target_price,
            "investment_thesis": analysis.investment_thesis,
            "catalysts": "\n".join(f"- {c}" for c in analysis.key_catalysts),
            "risks": "\n".join(f"- {r}" for r in analysis.risks),
            "financials": analysis.financial_highlights,
            "conclusion": analysis.conclusion
        })


class AnalysisOptimizer:
    """
    Refines analysis based on evaluation feedback.

    This is the "optimizer" in the evaluator-optimizer pattern.
    It improves the analysis iteratively based on critique.
    """

    def __init__(self, llm: Optional[ChatOpenAI] = None):
        """Initialize the optimizer."""
        self.llm = llm or create_chat_llm(temperature=0.3)

    def refine(
        self,
        original_analysis: InvestmentAnalysis,
        evaluation: QualityEvaluation,
        company_data: str
    ) -> InvestmentAnalysis:
        """
        Refine analysis based on evaluation feedback.

        Args:
            original_analysis: The original analysis
            evaluation: Quality evaluation with feedback
            company_data: Original company data

        Returns:
            Refined InvestmentAnalysis
        """
        structured_llm = self.llm.with_structured_output(InvestmentAnalysis)

        prompt = ChatPromptTemplate.from_messages([
            ("system", """You are refining an investment analysis based on expert feedback.

Your task:
1. Address all weaknesses identified
2. Add missing elements
3. Implement improvement suggestions
4. Expand areas that need more detail
5. Clarify ambiguous points

Keep the strengths, fix the weaknesses. Make the analysis significantly better."""),
            ("user", """Original Analysis:
Ticker: {ticker}
Recommendation: {recommendation}
Target Price: ${target_price}
Investment Thesis: {investment_thesis}

Evaluation Feedback:
Overall Score: {overall_score}/100
Strengths: {strengths}
Weaknesses: {weaknesses}
Missing Elements: {missing_elements}

Specific Feedback:
{specific_feedback}

Improvement Suggestions:
{improvements}

Company Data (for reference):
{company_data}

Provide a refined, improved analysis:""")
        ])

        chain = prompt | structured_llm
        return chain.invoke({
            "ticker": original_analysis.ticker,
            "recommendation": original_analysis.recommendation,
            "target_price": original_analysis.target_price,
            "investment_thesis": original_analysis.investment_thesis,
            "overall_score": evaluation.overall_score,
            "strengths": ", ".join(evaluation.strengths),
            "weaknesses": ", ".join(evaluation.weaknesses),
            "missing_elements": ", ".join(evaluation.missing_elements),
            "specific_feedback": evaluation.specific_feedback,
            "improvements": "\n".join(f"- {s}" for s in evaluation.improvement_suggestions),
            "company_data": company_data
        })


class EvaluatorOptimizerWorkflow:
    """
    Complete evaluator-optimizer workflow.

    Implements iterative refinement: Generate → Evaluate → Refine → Re-evaluate
    """

    def __init__(self, llm: Optional[ChatOpenAI] = None, max_iterations: int = 3):
        """
        Initialize the workflow.

        Args:
            llm: Language model to use
            max_iterations: Maximum refinement iterations
        """
        self.analyzer = InvestmentAnalyzer(llm)
        self.evaluator = AnalysisEvaluator(llm)
        self.optimizer = AnalysisOptimizer(llm)
        self.max_iterations = max_iterations
        self.quality_threshold = 75.0

    def run(self, ticker: str, company_data: str, verbose: bool = True) -> Dict:
        """
        Execute the complete evaluator-optimizer workflow.

        Args:
            ticker: Stock ticker
            company_data: Company and financial data
            verbose: Whether to print progress

        Returns:
            Dictionary with final analysis, evaluation history, and metadata
        """
        if verbose:
            print("\n=== EVALUATOR-OPTIMIZER WORKFLOW ===")

        iteration_history = []
        current_analysis = None
        current_evaluation = None

        for iteration in range(1, self.max_iterations + 1):
            if verbose:
                print(f"\n--- Iteration {iteration}/{self.max_iterations} ---")

            # Generate or refine analysis
            if iteration == 1:
                if verbose:
                    print("\n[1] Generating initial analysis...")
                current_analysis = self.analyzer.generate(ticker, company_data)
                if verbose:
                    print(f"✓ Initial analysis complete: {current_analysis.recommendation} "
                          f"recommendation, ${current_analysis.target_price} target")
            else:
                if verbose:
                    print(f"\n[1] Refining analysis based on feedback...")
                current_analysis = self.optimizer.refine(
                    current_analysis,
                    current_evaluation,
                    company_data
                )
                if verbose:
                    print(f"✓ Refined analysis: {current_analysis.recommendation} "
                          f"recommendation, ${current_analysis.target_price} target")

            # Evaluate
            if verbose:
                print(f"\n[2] Evaluating analysis quality...")
            current_evaluation = self.evaluator.evaluate(current_analysis, company_data)

            if verbose:
                print(f"✓ Quality Score: {current_evaluation.overall_score:.1f}/100")
                print(f"  - Completeness: {current_evaluation.completeness_score:.1f}")
                print(f"  - Accuracy: {current_evaluation.accuracy_score:.1f}")
                print(f"  - Actionability: {current_evaluation.actionability_score:.1f}")
                print(f"  - Acceptable: {current_evaluation.is_acceptable}")

            # Store iteration
            iteration_history.append({
                "iteration": iteration,
                "analysis": current_analysis,
                "evaluation": current_evaluation
            })

            # Check if we've met quality threshold
            if current_evaluation.overall_score >= self.quality_threshold:
                if verbose:
                    print(f"\n✓ Quality threshold met ({self.quality_threshold})! "
                          f"Analysis accepted.")
                break

            if iteration < self.max_iterations:
                if verbose:
                    print(f"\n  Quality below threshold. Refining...")
            else:
                if verbose:
                    print(f"\n⚠ Max iterations reached. Using best available analysis.")

        if verbose:
            print("\n=== WORKFLOW COMPLETE ===\n")

        return {
            "final_analysis": current_analysis,
            "final_evaluation": current_evaluation,
            "iteration_history": iteration_history,
            "iterations_performed": len(iteration_history),
            "quality_threshold_met": current_evaluation.overall_score >= self.quality_threshold
        }


def demonstrate_evaluator_optimizer():
    """Demonstration of the evaluator-optimizer workflow."""

    sample_data = """
    Tesla Inc. (TSLA)

    Recent Financial Performance:
    - Q3 2024 Revenue: $25.2B (+8% YoY)
    - Automotive Revenue: $20.0B
    - Energy & Services: $5.2B (+20% YoY)
    - Net Income: $1.85B
    - EPS: $0.53 (beat estimates of $0.46)
    - Operating Margin: 7.6% (down from 11.0% last year)
    - Free Cash Flow: $1.3B

    Recent Developments:
    - Price cuts across major models to stimulate demand
    - Cybertruck production ramping up, now at 1,000 units/week
    - Energy storage deployments up 73% YoY
    - FSD (Full Self-Driving) subscription revenue growing
    - Opening Gigafactory in Mexico delayed to 2025

    Market Metrics:
    - Current Price: $242
    - 52-week range: $152 - $299
    - P/E Ratio: 75
    - Market Cap: $765B

    Industry Context:
    - EV market growth slowing (was 40% YoY, now 15%)
    - Increased competition from legacy automakers
    - Federal EV tax credits under review
    """

    workflow = EvaluatorOptimizerWorkflow(max_iterations=3)

    print("\n" + "="*60)
    print("EVALUATOR-OPTIMIZER WORKFLOW DEMONSTRATION")
    print("Analyzing Tesla (TSLA) with iterative refinement")
    print("="*60)

    result = workflow.run("TSLA", sample_data, verbose=True)

    # Display final results
    print("\n" + "="*60)
    print("FINAL ANALYSIS RESULTS")
    print("="*60)

    final = result["final_analysis"]
    eval_final = result["final_evaluation"]

    print(f"\nTicker: {final.ticker}")
    print(f"Recommendation: {final.recommendation}")
    print(f"Target Price: ${final.target_price}")
    print(f"\nInvestment Thesis:\n{final.investment_thesis}")
    print(f"\nKey Catalysts:")
    for catalyst in final.key_catalysts:
        print(f"  • {catalyst}")
    print(f"\nKey Risks:")
    for risk in final.risks:
        print(f"  • {risk}")

    print(f"\n{'-'*60}")
    print("QUALITY METRICS")
    print(f"{'-'*60}")
    print(f"Final Score: {eval_final.overall_score:.1f}/100")
    print(f"Iterations: {result['iterations_performed']}")
    print(f"Threshold Met: {result['quality_threshold_met']}")

    print(f"\nScore Progression:")
    for item in result["iteration_history"]:
        score = item["evaluation"].overall_score
        print(f"  Iteration {item['iteration']}: {score:.1f}/100")

    return result


if __name__ == "__main__":
    demonstrate_evaluator_optimizer()
